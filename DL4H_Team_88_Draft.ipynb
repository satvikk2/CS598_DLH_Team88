{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlv6knX04FiY"
      },
      "source": [
        "# TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records.\n",
        "**CS598 Project Draft**\n",
        "\n",
        "Anikesh Haran - anikesh2@illinois.edu         \n",
        "Satvik Kulkarni - satvikk2@illinois.edu         \n",
        "Changhua Zhan - zhan36@illinois.edu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GitHub Repository\n",
        "\n",
        "https://github.com/satvikk2/CS598_DLH_Team88"
      ],
      "metadata": {
        "id": "rMDlGAStlKzU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The paper addresses the pressing need for accurate prediction of clinical diseases and outcomes using electronic health records (EHRs). Specifically, it focuses on the problem of disease prediction and outcome forecasting, which holds immense significance in enhancing patient care and healthcare management. This problem involves intricate feature engineering and data processing due to the complexity and interrelation of various diseases and outcomes. Additionally, the challenge lies in achieving high predictive accuracy amidst the vast and heterogeneous nature of EHR data. Traditional machine learning methods have been employed but are being outperformed by deep learning techniques.\n",
        "\n",
        "# State of the Art Methods\n",
        "The paper introduces TransformEHR, a novel denoising sequence to sequence transformer model, which tackles the limitations of existing methods. It innovatively pretrains on longitudinal EHRs to predict complete sets of ICD codes for future visits. The method's innovation lies in its generative encoder-decoder framework, which incorporates self-attention and cross-attention mechanisms. TransformEHR surpasses state-of-the-art BERT models, particularly excelling in predicting uncommon ICD codes.\n",
        "\n",
        "# TransformEHR\n",
        "The paper presents TransformEHR as a solution to the challenges in disease prediction and outcome forecasting. Its key innovation is the novel pretraining objective, which predicts all diseases and outcomes for future visits using longitudinal EHR data. Additionally, its generative encoder-decoder framework outperforms existing encoder-based models due to its attention mechanisms. TransformEHR achieves significant improvements in predicting both common and uncommon ICD codes, showcasing its effectiveness.\n",
        "\n",
        "We can add more details inlcuding architecture here.\n",
        "\n",
        "# Contribution to Research Regime\n",
        "The paper's contributions are multifaceted. Firstly, it proposes a new pretraining objective that captures complex interrelations among diseases and outcomes, addressing a critical gap in existing methods. Secondly, its innovative encoder-decoder framework sets a new standard for predictive modeling using EHRs, achieving superior performance compared to state-of-the-art methods. Thirdly, the study demonstrates the potential of TransformEHR in clinical screening and intervention, highlighting its practical significance. Overall, the paper significantly advances the field by offering a robust and effective solution to disease prediction and outcome forecasting using EHR data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "The reproducibility scope entails implementing and evaluating the TransformEHR model, a transformer-based encoder-decoder generative model specifically designed for disease outcome prediction using Electronic Health Records (EHRs). The model will undergo training on the MIMIC-IV dataset, consisting of deidentified patient records. The objective is to validate the model's capacity to learn meaningful representations and patterns associated with disease progression and outcomes using the provided dataset.\n",
        "\n",
        "**Hypotheses**\n",
        "\n",
        "- TransformEHR will achieve competitive performance compared to traditional machine learning models in predicting various disease outcomes using EHR data.\n",
        "- The pre-training objective employed in TransformEHR, specifically predicting all future\n",
        "diagnoses, will improve the model's generalizability to diverse clinical prediction tasks.\n",
        "- The model will effectively capture temporal dependencies and complex patterns within EHR\n",
        "data, leading to more accurate predictions.\n",
        "- We will strive to distill complex patterns learned by TransformEHR into interpretable insights\n",
        "for clinicians, while achieving interpretability is inherently challenging in deep learning\n",
        "models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "**Pretrain-Finetune Paradigm**\n",
        "\n",
        "The Pretrain-Finetune paradigm is a widely used strategy in deep learning that involves two distinct phases to train a model effectively. In the pretraining phase, the model is trained on a large dataset using unsupervised or self-supervised learning tasks, such as language modeling or image reconstruction. This phase aims to capture general patterns and features from the data domain, leveraging the vast amount of information available in the large dataset. The pretrained model learns rich representations and general knowledge, which can be transferred to various downstream tasks.\n",
        "\n",
        "Following pretraining, the finetuning phase involves adapting the pretrained model to a specific task or domain by fine-tuning its parameters using a smaller, domain-specific dataset with labeled data. This dataset is typically more focused on the target task, such as classification or sequence labeling. By finetuning on this dataset, the model refines its learned representations to better suit the nuances and intricacies of the specific task. The combination of pretraining on a large dataset and finetuning on a smaller task-specific dataset allows the model to leverage both general knowledge and task-specific information, leading to improved performance and robustness on the target task.\n",
        "\n",
        "**Transform EHR**\n",
        "\n",
        "**Step #1** - first TransformEHR is pre-trained with a generative encoder-decoder transformer on a large set of EHR data. TransformEHR will learn the probability distribution of ICD codes against random distribution through the correlation of cross attention.\n",
        "\n",
        "**Step #2** - in the downstream finetuning, TransformEHR predicts a single disease or outcome. Through the calculated attention weights above, TransformEHR is able to identify top indicators for the predictions. This is shown in the picture below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data\n",
        "\n",
        "The dataset we plan to use in this project is MIMIC-IV from https://physionet.org. The MIMIC-IV dataset includes intensive care unit patients admitted to the Beth Israel Deaconess Medical Center in Boston, Massachusetts, comprises deidentified patient records used for medical research and analysis. It encompasses a wide range of clinical data, including demographic information, vital signs, laboratory results, medications, procedures, and clinical notes. MIMIC-IV offers longitudinal Electronic Health Records (EHRs) from various healthcare facilities, providing a comprehensive view of patient health trajectories. This dataset serves as a valuable resource for studying disease progression, treatment outcomes, predictive modeling, and other healthcare-related research endeavors.\n",
        "\n",
        "Since the dataset contains information from 2008 to 2019 but the implementation of ICD-10CM started from October 2015, to mimic the same dataset as per the paper, we have converted ICD9CM codes into ICD10CM codes first to have enough patients and visits for the cohorts for pretraining, resulting in a dataset of 180733 patients.\n",
        "\n",
        "**Longitudnal EHR Data**\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1hqh-LCYG6wGxSgyyoih_b5ox7SfxBzg-)\n",
        "\n",
        "**Data Includes**\n",
        "\n",
        "* Raw Data - MIMIC IV tables\n",
        "  * Admitions,\n",
        "  * Patient and\n",
        "  * Icd_diagonosis codes\n",
        "\n",
        "* Descriptive Statistics\n",
        "\t- Dataset: MIMIC4Dataset\n",
        "\t- Number of patients: 180733\n",
        "\t- Number of visits: 431231\n",
        "\t- Number of visits per patient: 2.3860\n",
        "\t- Number of events per visit in diagnoses_icd: 11.0296\n",
        "  - Train and Valdiation set - TBD\n",
        "\n",
        "**Data Processing (feature engineering)**\n",
        "\n",
        "**MIMIC-IV Cohort**\n",
        "\n",
        "Our pretraining cohort comprises 180733 patients and 431231 admissions. As per the paper To evaluate pretrained models, we created two disease/outcome agnostic prediction (DOAP) datasets—one for common and one for uncommon diseases/outcomes. We selected 10 ICD-10CM codes with the highest prevalence (prevalence ratio >2%) in our pretraining cohort for our common disease/outcome DOAP dataset. As for the set of uncommon diseases/outcomes, we followed the FDA guidelines30 to randomly select 10 ICD-10CM codes with a prevalence ratio ranging from 0.04% to 0.05% in our pretraining cohort. The lists of common and uncommon diseases/outcomes are shown in Table 1.\n",
        "\n",
        "**Data Processing**\n",
        "\n",
        "For data pre-processing we have used PyHealth pyhealth.datasets.MIMIC4Dataset to process the unstructured raw data into a structured dataset object. See the implementation section below.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1cE_7Xbbp5NWFi-l8b2xs4fMURgOqNy-d)\n",
        "\n",
        "**Created Common & Uncommon DataSet**\n",
        "Extract Relevant Information: Extract the necessary information from the MIMIC-IV dataset, including patient records, diagnoses, and outcomes.\n",
        "\n",
        "* Identify Prevalent ICD-10CM Codes: Identify the prevalent ICD-10CM codes in the pretraining cohort. For the common disease/outcome DOAP dataset, select 10 ICD-10CM codes with the highest prevalence ratio (>2%) in the pretraining cohort.\n",
        "\n",
        "* Select Uncommon ICD-10CM Codes: Follow the FDA guidelines to randomly select 10 ICD-10CM codes with a prevalence ratio ranging from 0.04% to 0.05% in the pretraining cohort for the set of uncommon diseases/outcomes.\n",
        "\n",
        "* Create Common Disease/Outcome DOAP Dataset: Filter the patient records to include only those with the selected common ICD-10CM codes. This will form the common disease/outcome DOAP dataset.\n",
        "\n",
        "* Create Uncommon Disease/Outcome DOAP Dataset: Similarly, filter the patient records to include only those with the selected uncommon ICD-10CM codes. This will form the uncommon disease/outcome DOAP dataset.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=13eJz6spok4iTyYVkAA5dcJY17AwQf7PQ)\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1hs6G1u7rjXLUTMyLJ5MerLrs1g0WVQLc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install required packages\n",
        "!pip install pyhealth"
      ],
      "metadata": {
        "id": "pz2KVzNhnLYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "UrXjNkWlnN0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzVUQS0CHry0"
      },
      "source": [
        "**Load Data**\n",
        "\n",
        "The MIMIC-IV dataset, a valuable resource for healthcare research, comes with stringent data sharing restrictions designed to protect patient privacy and ensure ethical use. Access to MIMIC-IV necessitates signing a Data Use Agreement (DUA) with the MIT Laboratory for Computational Physiology, outlining terms such as authorized use, privacy protection measures, and attribution requirements.\n",
        "\n",
        "Since we are bound to not share the RAW data. We have pre-processed the raw data and created the pickle files for quick loading and model traninign. We have checked in the processed pickle files into GitHub under data folder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAW Data Processing**"
      ],
      "metadata": {
        "id": "7OO0aCNvs4wm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "\"\"\"from pyhealth.datasets import MIMIC4Dataset\n",
        "\n",
        "# dir and function to load raw data\n",
        "root = '/content/drive/MyDrive/DLH/MIMIC4/CSV/'\n",
        "\n",
        "def load_raw_data(raw_data_dir):\n",
        "  # implement this function to load raw data to dataframe/numpy array/tensor\n",
        "  mimic4_ds = MIMIC4Dataset(\n",
        "    # Argument 1: It specifies the data folder root.\n",
        "    root=raw_data_dir,\n",
        "\n",
        "    # Argument 2: The users need to input a list of raw table names (e.g., DIAGNOSES_ICD.csv, PROCEDURES_ICD.csv).\n",
        "    tables=[\"diagnoses_icd\"],\n",
        "    # Argument 3: This argument input a dictionary (key is the source code\n",
        "    # vocabulary and value is the target code vocabulary .\n",
        "    # Default is empty dict, which means the original code will be used.\n",
        "    # We will use ICD10 codes.\n",
        "    code_mapping={}\n",
        "    )\n",
        "  return mimic4_ds\n",
        "\n",
        "mimic4_ds = load_raw_data(root)\n",
        "\n",
        "mimic4_ds.info()\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsiGlnvDZ3g6",
        "outputId": "41e5df69-fc2c-4596-a61f-68a027db12ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Statistics of base dataset (dev=False):\n",
            "\t- Dataset: MIMIC4Dataset\n",
            "\t- Number of patients: 180733\n",
            "\t- Number of visits: 431231\n",
            "\t- Number of visits per patient: 2.3860\n",
            "\t- Number of events per visit in diagnoses_icd: 11.0296\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['diagnoses_icd']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\"\"\"# Statistics of the entire dataset.\n",
        "mimic4_ds.stat()\n",
        "\n",
        "# You can find the list of all available tables in this dataset as\n",
        "mimic4_ds.available_tables\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"#Save data object to drive for quick retrival\n",
        "import pickle\n",
        "\n",
        "# Assuming your data object is named 'data_object'\n",
        "mimic4_ds_object_path = '/content/drive/MyDrive/DLH/MIMIC4/PKL/mimic4_ds.pkl'\n",
        "\n",
        "# Save the data object to Google Drive\n",
        "with open(mimic4_ds_object_path, 'wb') as f:\n",
        "    pickle.dump(mimic4_ds, f)\"\"\""
      ],
      "metadata": {
        "id": "MpVIqMZCrJQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"#Load MIMIC4 data from google drive\n",
        "import pickle\n",
        "\n",
        "# Path to the saved data object\n",
        "data_object_path = '/content/drive/MyDrive/DLH/MIMIC4/PKL/mimic4_ds.pkl'\n",
        "\n",
        "# Load the data object from Google Drive\n",
        "with open(data_object_path, 'rb') as f:\n",
        "    mimic4_data = pickle.load(f)\n",
        "\n",
        "# Statistics of the entire dataset.\n",
        "mimic4_data.stat()\n",
        "\n",
        "# You can find the list of all available tables in this dataset as\n",
        "mimic4_data.available_tables\"\"\""
      ],
      "metadata": {
        "id": "ISDV4zCosDOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Data**\n"
      ],
      "metadata": {
        "id": "Pmfx3saqstrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# get patient dictionary\n",
        "patient_dict = mimic4_data.patients\n",
        "print(list(patient_dict.keys())[:10])\n",
        "\n",
        "# get the \"10000032\" patient\n",
        "patient = patient_dict[\"10000032\"]\n",
        "print(patient)\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-Vt91of77_4",
        "outputId": "b1fce938-2df7-4194-f883-844cfe134cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['10000032', '10000068', '10000084', '10000108', '10000117', '10000248', '10000280', '10000560', '10000635', '10000719']\n",
            "Patient 10000032 with 4 visits:\n",
            "\t- Birth datetime: 2128-04-07 00:00:00\n",
            "\t- Death datetime: 2180-09-09 00:00:00\n",
            "\t- Gender: F\n",
            "\t- Ethnicity: WHITE\n",
            "\t- anchor_year_group: 2014 - 2016\n",
            "\t- Visit 22595853 from patient 10000032 with 8 events:\n",
            "\t\t- Encounter time: 2180-05-06 22:23:00\n",
            "\t\t- Discharge time: 2180-05-07 17:15:00\n",
            "\t\t- Discharge status: 0\n",
            "\t\t- Available tables: ['diagnoses_icd']\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: 5723\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: 78959\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: 5715\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: 07070\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: 496\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: 29680\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: 30981\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: V1582\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t- Visit 22841357 from patient 10000032 with 8 events:\n",
            "\t\t- Encounter time: 2180-06-26 18:27:00\n",
            "\t\t- Discharge time: 2180-06-27 18:49:00\n",
            "\t\t- Discharge status: 0\n",
            "\t\t- Available tables: ['diagnoses_icd']\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: 07071\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: 78959\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: 2875\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: 2761\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: 496\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: 5715\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: V08\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: 3051\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t- Visit 25742920 from patient 10000032 with 10 events:\n",
            "\t\t- Encounter time: 2180-08-05 23:44:00\n",
            "\t\t- Discharge time: 2180-08-07 17:50:00\n",
            "\t\t- Discharge status: 0\n",
            "\t\t- Available tables: ['diagnoses_icd']\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 07054\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 78959\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: V462\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 5715\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 2767\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 2761\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 496\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: V08\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 3051\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 78791\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t- Visit 29079034 from patient 10000032 with 13 events:\n",
            "\t\t- Encounter time: 2180-07-23 12:35:00\n",
            "\t\t- Discharge time: 2180-07-25 17:55:00\n",
            "\t\t- Discharge status: 0\n",
            "\t\t- Available tables: ['diagnoses_icd']\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 45829\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 07044\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 7994\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 2761\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 78959\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 2767\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 3051\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: V08\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: V4986\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: V462\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 496\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 29680\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 5715\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Common and Uncommon disease/outcome agnostic prediction (DOAP) datasets.**"
      ],
      "metadata": {
        "id": "akNjth33t44L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import random\n",
        "from collections import Counter\n",
        "\n",
        "# Step 1: Calculate the prevalence of each ICD-10CM code\n",
        "icd_counter = Counter()\n",
        "\n",
        "for patient in mimic4_sample:\n",
        "    for icd_code in patient['icd_codes']:\n",
        "        icd_counter[icd_code] += 1\n",
        "\n",
        "# Step 2: Select top 10 ICD-10CM codes with highest prevalence ratio (>2%) for common dataset\n",
        "# common_icd_codes = [icd_code for icd_code, count in icd_counter.items() if (count / total_patients) > 0.02][:10]\n",
        "# NOTE: we need diseases with the top 10 prevalence ratio\n",
        "common_icd_codes = [icd_code for icd_code, count in icd_counter.most_common(10)]\n",
        "# check whether all selected diseases has a prevalence ratio of > 2%\n",
        "print(sum([count/total_patients > 0.02 for icd_code, count in icd_counter.items() if icd_code in common_icd_codes]))\n",
        "\n",
        "\n",
        "# Step 3: Randomly select 10 ICD-10CM codes with prevalence ratio ranging from 0.04% to 0.05% for uncommon dataset\n",
        "uncommon_icd_codes = [icd_code for icd_code, count in icd_counter.items() if 0.0004 <= (count / total_patients) <= 0.0005]\n",
        "random.shuffle(uncommon_icd_codes)\n",
        "uncommon_icd_codes = uncommon_icd_codes[:10]\n",
        "\n",
        "# Step 4: Filter patient records to create common and uncommon datasets\n",
        "common_disease_dataset = [patient for patient in mimic4_sample if any(icd in patient['icd_codes'] for icd in common_icd_codes)]\n",
        "uncommon_disease_dataset = [patient for patient in mimic4_sample if any(icd in patient['icd_codes'] for icd in uncommon_icd_codes)]\n",
        "\n",
        "# Print the selected ICD-10CM codes for common and uncommon datasets\n",
        "print(\"Selected Common ICD-10CM Codes:\", common_icd_codes)\n",
        "print(\"Selected Uncommon ICD-10CM Codes:\", uncommon_icd_codes)\n",
        "\n",
        "# Optionally, print the lengths of the resulting datasets\n",
        "print(\"Number of patients in Common Disease/Outcome DOAP Dataset:\", len(common_disease_dataset))\n",
        "print(\"Number of patients in Uncommon Disease/Outcome DOAP Dataset:\", len(uncommon_disease_dataset))\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2gVo3MxDRqy",
        "outputId": "1c0caccd-240a-4a70-9551-52de97f26fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "Selected Common ICD-10CM Codes: ['I10', 'E785', 'Z87891', 'K219', 'F329', 'I2510', 'F419', 'N179', 'Z794', 'Z7901']\n",
            "Selected Uncommon ICD-10CM Codes: ['N94.6', 'T47.1X5D', 'O30.033', 'I70234', 'I95.2', 'Z34.83', 'C8518', 'L89.891', 'D126', 'I201']\n",
            "Number of patients in Common Disease/Outcome DOAP Dataset: 133711\n",
            "Number of patients in Uncommon Disease/Outcome DOAP Dataset: 340\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lists of common and uncommon diseases/outcomes are shown in Table 1 and Table 2 respectivly.\n",
        "\n",
        "**Table 1 - Common ICD-10CM Codes**"
      ],
      "metadata": {
        "id": "sLVqo3e5nIbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the data for the table\n",
        "common_outcomes = {\n",
        "    'ICD-10-CM Code': ['I10', 'E785', 'Z87891', 'K219', 'F329', 'I2510', 'F419', 'N179', 'Z794', 'Z7901'],\n",
        "    'Description': [\n",
        "        'Essential (primary) hypertension',\n",
        "        'Hyperlipidemia, unspecified',\n",
        "        'Personal history of nicotine dependence',\n",
        "        'Gastro-esophageal reflux disease without esophagitis',\n",
        "        'Major depressive disorder, unspecified',\n",
        "        'Atherosclerotic heart disease of native coronary artery without angina pectoris',\n",
        "        'Unspecified anxiety disorder',\n",
        "        'Chronic kidney disease, unspecified',\n",
        "        'Long-term (current) use of insulin',\n",
        "        'Long-term (current) use of opiate analgesic'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "common_outcomes_df = pd.DataFrame(common_outcomes)\n",
        "\n",
        "# Display the DataFrame\n",
        "common_outcomes_df"
      ],
      "metadata": {
        "id": "ROocXt6AnAo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Table 2 - Uncommon ICD-10CM Codes**"
      ],
      "metadata": {
        "id": "4KWlL4n4ny32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the data for the table\n",
        "uncommon_outcomes = {\n",
        "    'ICD-10-CM Code': ['N94.6', 'T47.1X5D', 'O30.033', 'I70234', 'I95.2', 'Z34.83', 'C8518', 'L89.891', 'D126', 'I201'],\n",
        "    'Description': [\n",
        "        'Dyspareunia, unspecified',\n",
        "        'Poisoning by antineoplastic and immunosuppressive drugs, accidental (unintentional), subsequent encounter',\n",
        "        'Triplet pregnancy, fetus 3',\n",
        "        'Atherosclerosis of native arteries of extremities with gangrene, bilateral legs',\n",
        "        'Hypotension, unspecified',\n",
        "        'Supervision of high-risk pregnancy with other poor reproductive or obstetric history',\n",
        "        'Diffuse large B-cell lymphoma, lymph nodes of axilla and upper limb',\n",
        "        'Pressure ulcer of other site, stage 1',\n",
        "        'Benign neoplasm of colon',\n",
        "        'Unstable angina'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the additional data\n",
        "uncommon_outcomes_df = pd.DataFrame(uncommon_outcomes)\n",
        "\n",
        "# Display the DataFrame\n",
        "uncommon_outcomes_df"
      ],
      "metadata": {
        "id": "Zbw08zfsoKrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Patient Age - PreProcessing**\n",
        "\n",
        "Current PyHealth based data processing does not compute age feature. hence we pre-processed the patient's age separately and created a pickle files for age feature for quick loading during model training."
      ],
      "metadata": {
        "id": "lAi97MBsuOXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import csv\n",
        "\n",
        "# Define the path to the CSV file\n",
        "root = '/content/drive/MyDrive/DLH/MIMIC4/CSV/'\n",
        "patient_file_path = root + 'patients.csv'\n",
        "\n",
        "id2age = {}\n",
        "\n",
        "# read id and age from patients.csv and save it in a dictionary id2age\n",
        "def read_patient_age(file_path):\n",
        "    with open(file_path, mode='r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            id2age[row[0]] = int(row[2])\n",
        "\n",
        "read_patient_age(patient_file_path)\"\"\"\n"
      ],
      "metadata": {
        "id": "zEko0w_z-KF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-Processing - transform_ehr_mimic4_fn**\n",
        "\n",
        "We have developed function **transform_ehr_mimic4_fn** to process individual patients and create feautres such as visit level details, icd codes and patinet;s demographic details such as age, gender and race."
      ],
      "metadata": {
        "id": "4Cx3SNQfuu-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Compute sequenced data for learning embeddings\n",
        "\n",
        "from datetime import datetime\n",
        "from pyhealth.medcode import CrossMap\n",
        "import random\n",
        "# set the random seed\n",
        "random.seed(0)\n",
        "\n",
        "# load the mapping from ICD9CM to CCSCM\n",
        "mapping_icd9cm_ccscm = CrossMap.load(source_vocabulary=\"ICD9CM\", target_vocabulary=\"CCSCM\")\n",
        "# load the mapping from CCSCM to ICD10CM\n",
        "mapping_ccscm_icd10cm = CrossMap.load(source_vocabulary=\"CCSCM\", target_vocabulary=\"ICD10CM\")\n",
        "\n",
        "#Calculate Patient's Age\n",
        "def calculate_age(birth_date, death_date):\n",
        "  # Calculate age\n",
        "  print(birth_date, death_date)\n",
        "  age = death_date.year - birth_date.year - ((death_date.month, death_date.day) < (birth_date.month, birth_date.day))\n",
        "  return age\n",
        "\n",
        "types = {}\n",
        "gender2idx = {}\n",
        "race2idx = {}\n",
        "age = []\n",
        "gender = []\n",
        "race = []\n",
        "visit_dates = []\n",
        "\n",
        "def transform_ehr_mimic4_fn(patient):\n",
        "    \"\"\"\n",
        "    types = {}\n",
        "\tnewSeqs = []\n",
        "\tfor patient in seqs:\n",
        "\t\tnewPatient = []\n",
        "\t\tfor visit in patient:\n",
        "\t\t\tnewVisit = []\n",
        "\t\t\tfor code in visit:\n",
        "\t\t\t\tif code in types:\n",
        "\t\t\t\t\tnewVisit.append(types[code])\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\ttypes[code] = len(types)\n",
        "\t\t\t\t\tnewVisit.append(types[code])\n",
        "\t\t\tnewPatient.append(newVisit)\n",
        "\t\tnewSeqs.append(newPatient)\n",
        "\n",
        "    \"\"\"\n",
        "    newPatient = []\n",
        "    visit_date = []\n",
        "\n",
        "    for i in range(len(patient)):\n",
        "\n",
        "      visit = patient[i]\n",
        "      formatted_visit_date = visit.encounter_time.strftime(\"%Y-%m-%d\")\n",
        "      visit_date.append(formatted_visit_date)\n",
        "\n",
        "      conditions = []\n",
        "\n",
        "      events = visit.get_event_list(table=\"diagnoses_icd\")\n",
        "      for event in events:\n",
        "        vocabulary = event.vocabulary\n",
        "        code = \"\"\n",
        "        if vocabulary == \"ICD9CM\":\n",
        "          # map from ICD9CM to CCSCM\n",
        "          ccscmCodes = mapping_icd9cm_ccscm.map(event.code)\n",
        "          # in the case where one ICD9CM code maps to multiple CCSCM codes, randomly select one\n",
        "          ccscmCode = random.choice(ccscmCodes)\n",
        "\n",
        "          # map from CCSCM to ICD10CM\n",
        "          icd10cmCodes = mapping_ccscm_icd10cm.map(ccscmCode)\n",
        "          # in the case where one CCSCM code maps to multiple ICD10CM codes, randomly select one\n",
        "          code = random.choice(icd10cmCodes)\n",
        "        else:\n",
        "          code = event.code\n",
        "\n",
        "        if code in types:\n",
        "          conditions.append(types[code])\n",
        "        else:\n",
        "          types[code] = len(types)\n",
        "          conditions.append(types[code])\n",
        "\n",
        "      # step 2: assemble the sample\n",
        "      # if conditions is not empty, add the sample\n",
        "      # if (conditions): # commented it out because len(visit_date) needs to be the same as len(newPatient)\n",
        "      newPatient.append(conditions)\n",
        "\n",
        "    if len(newPatient) > 100:\n",
        "      print(patient.patient_id,)\n",
        "    visit_dates.append(visit_date)\n",
        "    #age.append(patient.anchor_age)\n",
        "\n",
        "    # get age of patient using patient id and id2age dictionary\n",
        "    age.append(id2age[patient.patient_id])\n",
        "\n",
        "    p_gender = patient.gender\n",
        "    if p_gender in gender2idx:\n",
        "      gender.append(gender2idx[p_gender])\n",
        "    else:\n",
        "      gender2idx[p_gender] = len(gender2idx)\n",
        "      gender.append(gender2idx[p_gender])\n",
        "\n",
        "    p_ethnicity = patient.ethnicity\n",
        "    if p_ethnicity in race2idx:\n",
        "      race.append(race2idx[p_ethnicity])\n",
        "    else:\n",
        "      race2idx[p_ethnicity] = len(race2idx)\n",
        "      race.append(race2idx[p_ethnicity])\n",
        "    return newPatient\"\"\""
      ],
      "metadata": {
        "id": "d9migu3ZD41n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ordering Visits Based on Visit Dates**"
      ],
      "metadata": {
        "id": "34gRz8KLvkDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# sort the visit_date and seqs based on the visit date\n",
        "sorted_seqs = []\n",
        "sorted_visit_dates = []\n",
        "for i in range(len(seqs)):\n",
        "    visit_date = visit_dates[i]\n",
        "    seq = seqs[i]\n",
        "    visit_date_seq_tuple = [(visit_date[j], seq[j]) for j in range(len(seq))]\n",
        "    visit_date_seq_tuple.sort(key=lambda x: datetime.strptime(x[0], \"%Y-%m-%d\"))\n",
        "\n",
        "    sorted_visit_dates.append([x[0] for x in visit_date_seq_tuple])\n",
        "    sorted_seqs.append([x[1] for x in visit_date_seq_tuple])\n",
        "\n",
        "seqs = sorted_seqs\n",
        "visit_dates = sorted_visit_dates\n",
        "print(seqs[0])\n",
        "print(visit_dates[0])\"\"\""
      ],
      "metadata": {
        "id": "BlxJmKFnkc1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(visit_dates[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_FZmHrl1zXb",
        "outputId": "60f05b83-985a-49a2-80e5-89c550c77ca1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['2180-05-06', '2180-06-26', '2180-08-05', '2180-07-23']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Pickle**\n",
        "\n",
        "In below code section we have created pickle files for all the data features - sequences, visit dates, gender , race & age and stored into Drive."
      ],
      "metadata": {
        "id": "YXXCCW20wZht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import pickle\n",
        "\n",
        "# Assuming your data object is named 'data_object'\n",
        "mimic4_ds_seqs_path = 'seqs.pkl'\n",
        "mimic4_ds_visit_dates_path = 'dates.pkl'\n",
        "mimic4_ds_type_path = 'type.pkl'\n",
        "mimic4_ds_gender_path = 'gender.pkl'\n",
        "mimic4_ds_race_path = 'race.pkl'\n",
        "mimic4_ds_age_path = 'age.pkl'\n",
        "\n",
        "# Save the data object to Google Drive\n",
        "with open(mimic4_ds_seqs_path, 'wb') as f:\n",
        "    pickle.dump(seqs, f)\n",
        "\n",
        "with open(mimic4_ds_visit_dates_path, 'wb') as f:\n",
        "    pickle.dump(visit_dates, f)\n",
        "\n",
        "with open(mimic4_ds_type_path, 'wb') as f:\n",
        "    pickle.dump(types, f)\n",
        "\n",
        "with open(mimic4_ds_gender_path, 'wb') as f:\n",
        "    pickle.dump(gender, f)\n",
        "\n",
        "with open(mimic4_ds_race_path, 'wb') as f:\n",
        "    pickle.dump(race, f)\n",
        "\n",
        "with open(mimic4_ds_age_path, 'wb') as f:\n",
        "    pickle.dump(age, f)\"\"\""
      ],
      "metadata": {
        "id": "rBL_g0JZ5yD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data for Model Training**"
      ],
      "metadata": {
        "id": "W8VYPfaDxA-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load MIMIC4 data from google drive\n",
        "import pickle\n",
        "\n",
        "# Path to the saved data object\n",
        "# Assuming your data object is named 'data_object'\n",
        "mimic4_ds_seqs_path = 'seqs.pkl'\n",
        "mimic4_ds_visit_dates_path = 'dates.pkl'\n",
        "mimic4_ds_type_path = 'type.pkl'\n",
        "mimic4_ds_gender_path = 'gender.pkl'\n",
        "mimic4_ds_race_path = 'race.pkl'\n",
        "mimic4_ds_age_path = 'age.pkl'\n",
        "\n",
        "# Load the data object from Google Drive\n",
        "with open(mimic4_ds_seqs_path, 'rb') as f:\n",
        "    seqs = pickle.load(f)\n",
        "\n",
        "# Load the data object from Google Drive\n",
        "with open(mimic4_ds_visit_dates_path, 'rb') as f:\n",
        "    visit_dates = pickle.load(f)\n",
        "\n",
        "# Load the data object from Google Drive\n",
        "with open(mimic4_ds_type_path, 'rb') as f:\n",
        "    icd_codes_types = pickle.load(f)\n",
        "\n",
        "# Load the data object from Google Drive\n",
        "with open(mimic4_ds_gender_path, 'rb') as f:\n",
        "    gender = pickle.load(f)\n",
        "\n",
        "# Load the data object from Google Drive\n",
        "with open(mimic4_ds_race_path, 'rb') as f:\n",
        "    race = pickle.load(f)\n",
        "\n",
        "# Load the data object from Google Drive\n",
        "with open(mimic4_ds_age_path, 'rb') as f:\n",
        "    age = pickle.load(f)"
      ],
      "metadata": {
        "id": "TP5Vc5mDGuUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(seqs))\n",
        "print(len(visit_dates))\n",
        "print(len(gender))\n",
        "print(len(race))\n",
        "print(len(age))\n",
        "print(len(icd_codes_types))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIjK2tXfZM7Y",
        "outputId": "dfde58a6-bd3b-4ae0-e0d1-7b231edb6423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "180733\n",
            "180733\n",
            "180733\n",
            "180733\n",
            "180733\n",
            "71273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build The Dataset**\n",
        "\n",
        "First, we have implement a custom dataset using PyTorch class Dataset, which will characterize the key features of the dataset we want to generate.\n",
        "\n",
        "We will use the sequences of diagnosis codes seqs, gender, age, race and visit-dates as input for pretraning."
      ],
      "metadata": {
        "id": "CKBc3MUxxXfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, seqs, gender, race, age, visit_dates):\n",
        "    self.x = seqs\n",
        "    self.gender = gender\n",
        "    self.race = race\n",
        "    self.age = age\n",
        "    self.visit_dates = visit_dates\n",
        "\n",
        "  def __len__(self):\n",
        "    # your code here\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Extract the sequence\n",
        "    sequence = self.x[index]\n",
        "    gender = self.gender[index]\n",
        "    race = self.race[index]\n",
        "    age = self.age[index]\n",
        "    visit_dates = self.visit_dates[index]\n",
        "    # Return the pair (sequence, hf)\n",
        "    print(sequence, gender, race, age, visit_dates)\n",
        "    return (sequence, gender, race, age, visit_dates)\n",
        "\n",
        "dataset = CustomDataset(seqs, gender, race, age, visit_dates)"
      ],
      "metadata": {
        "id": "dibp-dQ5qBRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.__getitem__(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRuDt1jZmqzm",
        "outputId": "b2ac3eea-762c-46f7-b167-47c56160fd69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 20, 4, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 14, 31, 32, 4, 33, 34]] 0 0 52 ['2180-05-06', '2180-06-26', '2180-08-05', '2180-07-23']\n",
            "([[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 20, 4, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 14, 31, 32, 4, 33, 34]], 0, 0, 52, ['2180-05-06', '2180-06-26', '2180-08-05', '2180-07-23'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Data Into Train and Validation Set**\n",
        "\n",
        "Now we have CustomDataset. Let us split the dataset into training and validation sets."
      ],
      "metadata": {
        "id": "OY-uRdGyuJ1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "split = int(len(dataset)*0.8)\n",
        "\n",
        "lengths = [split, len(dataset) - split]\n",
        "train_dataset, val_dataset = random_split(dataset, lengths)\n",
        "\n",
        "print(\"Length of train dataset:\", len(train_dataset))\n",
        "print(\"Length of val dataset:\", len(val_dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NZYwtzRtw2v",
        "outputId": "b7963161-08dc-468e-fad0-308466f72a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train dataset: 144586\n",
            "Length of val dataset: 36147\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loader & collate_fn Implementation**\n",
        "\n",
        "Within collate_fu we are computing positional encoding to embed the time, we applied sinusoidal position embedding [2] to the numerical format of visit date (date-specific)"
      ],
      "metadata": {
        "id": "fzQ4EJ1GyAZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "\n",
        "def load_data(dataset, batch_size):\n",
        "  def collate_fn(data):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "        data: a list of samples fetched from `CustomDataset`\n",
        "    Outputs:\n",
        "        x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
        "        masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
        "        gender: a tensor of shape (# patients) of type torch.long (optional)\n",
        "        race: a tensor of shape (# patients) of type torch.long (optional)\n",
        "        visit_dates: a list of lists of strings representing visit dates (optional)\n",
        "    \"\"\"\n",
        "    def get_position_encoding(position, d_model):\n",
        "      \"\"\"Calculates sinusoidal position encoding for a given position and embedding dimension.\"\"\"\n",
        "      pe = torch.zeros(d_model)\n",
        "      div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "      pe[0::2] = torch.sin(position * div_term)\n",
        "      pe[1::2] = torch.cos(position * div_term)\n",
        "      return pe.unsqueeze(0)\n",
        "\n",
        "    sequences, gender, race, age, visit_dates = zip(*data)\n",
        "    # Convert gender and race to tensors (optional)\n",
        "    if gender is not None:\n",
        "      gender = torch.tensor(gender, dtype=torch.long)\n",
        "    if race is not None:\n",
        "      race = torch.tensor(race, dtype=torch.long)\n",
        "    if age is not None:\n",
        "      age = torch.tensor(age, dtype=torch.long)\n",
        "\n",
        "    d_model = 2\n",
        "    num_patients = len(sequences)\n",
        "    num_visits = [len(patient) for patient in sequences]\n",
        "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
        "    max_num_visits = max(num_visits)\n",
        "    max_num_codes = max(num_codes)\n",
        "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
        "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
        "    position_encodings = torch.zeros((num_patients, max_num_visits, d_model), dtype=torch.float)  # For position encoding\n",
        "\n",
        "    for i_patient, (patient, visit_date) in enumerate(zip(sequences, visit_dates)):\n",
        "      for j_visit, visit in enumerate(patient):\n",
        "        # Mask all ICD codes in the visit\n",
        "        masked_visit = [0] * len(visit)  # Replace with actual masking logic (e.g., random masking)\n",
        "        padded_seq = torch.tensor(masked_visit + [0] * (max_num_codes - len(visit)), dtype=torch.long)\n",
        "        x[i_patient, j_visit, :] = padded_seq\n",
        "        masks[i_patient, j_visit, :] = torch.ones(max_num_codes, dtype=torch.bool)  # All codes masked in this visit\n",
        "        # Calculate position encoding based on visit date (assuming YYYY-MM-DD format)\n",
        "        year, month, day = map(int, visit_date[j_visit].split('-'))\n",
        "        # You can customize the date processing logic based on your data format\n",
        "        date_as_float = year + (month - 1) / 12 + day / (365.25 * 12)  # Approximate date as float\n",
        "        position_encodings[i_patient, j_visit, :] = get_position_encoding(date_as_float, d_model)\n",
        "\n",
        "    return (x, masks, gender, race, age, position_encodings)\n",
        "\n",
        "  return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "Tz9SntGruSkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = load_data(train_dataset, batch_size = 32)\n",
        "val_loader = load_data(val_dataset,  batch_size = 32)"
      ],
      "metadata": {
        "id": "fIab-GilWweU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the loader and collate function implementation\n",
        "loader_iter = iter(val_loader)\n",
        "x, masks, gender, race, age, position_encodings = next(loader_iter)\n",
        "print(x, masks, gender, race, age, position_encodings)"
      ],
      "metadata": {
        "id": "9xc2-EJdpMru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"x\", x.shape)\n",
        "print(\"masks\", masks.shape)\n",
        "print(\"gender\", gender.shape)\n",
        "print(\"race\", race.shape)\n",
        "print(\"age\", age.shape)\n",
        "print(\"position_encodings\", position_encodings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW6QJ6pXPM7Y",
        "outputId": "cf562310-bae4-48aa-e4a0-c8251bb5eb06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x torch.Size([32, 10, 30])\n",
            "masks torch.Size([32, 10, 30])\n",
            "gender torch.Size([32])\n",
            "race torch.Size([32])\n",
            "age torch.Size([32])\n",
            "position_encodings torch.Size([32, 10, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model\n",
        "### TransformEHR Model Architecture\n",
        "\n",
        "TransformEHR uses a encoder-decoder architecture. The encoder takes in visit, time, and code/demographic embeddings and generates a set of hidden representations for each predictor. TransformEHR then calculates cross-attention over the encoder's created hidden representation. From there, these weighted representations are sent into the decoder, which then creates the ICD codes of the future visit. The decoder generates ICD codes in sequential order of code priority. so for example, we see a primary diagnosis and secondary diagnosis based on primary diagnosis. This process is continued until all diagnoses of a future visit are completed. This process is shown in the picture below.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1kyMUMOtLsFbM72MKnfe1tyMshlJiHZIn)\n",
        "\n",
        "### Pretraining Step\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1lW3i3PYLUlNgv8GoFXBHjQlZZq8dqDEL)\n",
        "\n",
        "### Finetuning Step\n",
        "![](https://drive.google.com/uc?export=view&id=1kbcUNFOookyk6ohFj1gjTolQ_-BgnpyP)\n"
      ],
      "metadata": {
        "id": "gXMBlEqqy9aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TransformEHR Model\n",
        "\n",
        "This implementation of the TranformEHR model is designed for processing electronic health record (EHR) data. Here's a summary of the key components and functionalities:\n",
        "\n",
        "####Embedding Layers:\n",
        "\n",
        "* Embedding layers are used for categorical features such as gender and race.\n",
        "* Continuous features like age and position encodings are also embedded using linear layers.\n",
        "* Visit embeddings are obtained using an embedding layer based on the number of diagnosis codes.\n",
        "\n",
        "####Concatenation of Embeddings:\n",
        "\n",
        "* All embeddings (gender, race, age, position encodings, and visit embeddings) are concatenated along the feature dimension.\n",
        "* The concatenated embeddings are projected to a lower-dimensional space using a linear layer (embedding_projection).\n",
        "\n",
        "####Transformer Encoder:\n",
        "\n",
        "* Utilizes a transformer encoder with specified parameters like the number of encoder layers (num_encoder_layers) and the number of attention heads (nhead).\n",
        "* The encoder processes the concatenated embeddings.\n",
        "\n",
        "####Transformer Decoder:\n",
        "\n",
        "* Employs a transformer decoder with parameters such as the number of decoder layers (num_decoder_layers) and attention heads (nhead).\n",
        "* Takes the encoder output and the concatenated embeddings as inputs, with masking applied as needed.\n",
        "\n",
        "####Linear Layer for Output:\n",
        "\n",
        "*A linear layer (linear) is used to project the decoder output to predict probabilities for ICD codes.\n",
        "\n",
        "####Forward Method:\n",
        "\n",
        "* The forward method takes input data (x), masks for padding (masks), as well as gender, race, age, and position encodings.\n",
        "* It performs the embedding, concatenation, projection, transformer encoding, decoding, and output projection steps.\n",
        "\n",
        "####Model Initialization:\n",
        "\n",
        "* The model is initialized with specified parameters such as the number of gender classes, race classes, and the maximum number of visits and diagnosis codes.\n",
        "\n",
        "Overall, this implementation encapsulates the key components of the TranformEHR model for processing EHR data with transformer-based encoder-decoder architecture and cross attentions."
      ],
      "metadata": {
        "id": "rtNbQcMp1TE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of classes for each categorical feature\n",
        "num_gender_classes = 2\n",
        "num_race_classes = 33\n",
        "# Define the maximum number of visits and diagnosis codes\n",
        "max_num_visits = 18\n",
        "max_num_codes = 35\n",
        "\n",
        "class TranformEHR(nn.Module):\n",
        "  def __init__(self, num_gender_classes, num_race_classes, num_code, nhead, num_encoder_layers, num_decoder_layers, embedding_dim=128):\n",
        "    super().__init__()\n",
        "    # HZ\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.concatenated_dim = embedding_dim * 5\n",
        "    self.projected_dim = embedding_dim\n",
        "\n",
        "\n",
        "    # Define the embedding layers\n",
        "    self.gender_embedding = nn.Embedding(num_gender_classes, embedding_dim)\n",
        "    self.race_embedding = nn.Embedding(num_race_classes, embedding_dim)\n",
        "    # Define the embeddings for other continuous features (age, position_encodings)\n",
        "    self.age_embedding = nn.Linear(1, embedding_dim)  # Assuming age is a continuous feature\n",
        "    self.position_encodings_embedding = nn.Linear(2, embedding_dim)  # Assuming position_encodings has 2 dimensions\n",
        "    self.visit_embedding = nn.Embedding(num_embeddings=num_code, embedding_dim=embedding_dim)\n",
        "\n",
        "    # HZ\n",
        "    # self.embedding_projection = nn.Linear(embedding_dim * 5, self.projected_dim)\n",
        "    self.embedding_projection = nn.Linear(self.concatenated_dim, self.projected_dim)\n",
        "\n",
        "    # Transformer encoder\n",
        "    encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead)\n",
        "    self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "    # Transformer decoder\n",
        "    # decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=nhead)\n",
        "    decoder_layer = nn.TransformerDecoderLayer(d_model=self.projected_dim, nhead=nhead)\n",
        "\n",
        "    self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "    # Linear layer to project decoder output to ICD code probabilities\n",
        "    # self.linear = nn.Linear(embedding_dim, num_code)\n",
        "    self.linear = nn.Linear(self.projected_dim, num_code)\n",
        "\n",
        "  def forward(self, x, masks, gender, race, age, position_encodings):\n",
        "    embedded_gender = self.gender_embedding(gender)\n",
        "    embedded_race = self.race_embedding(race)\n",
        "    #embedded_age = self.age_embedding(age.float())\n",
        "    # HZ\n",
        "    # embedded_age = self.age_embedding(age.view(-1, 1).float())\n",
        "    embedded_age = self.age_embedding(age.float().unsqueeze(-1))\n",
        "    embedded_positional_encodings = self.position_encodings_embedding(position_encodings.float())\n",
        "    embedded_x = self.visit_embedding(x)\n",
        "\n",
        "    # Concatenate all embeddings\n",
        "    print(embedded_x.shape)\n",
        "    print(embedded_positional_encodings.shape)\n",
        "    print(embedded_age.shape)\n",
        "    print(embedded_race.shape)\n",
        "    print(embedded_gender.shape)\n",
        "    # HZ\n",
        "    # Add dimensions to positional encodings and other embeddings\n",
        "    # embedded_positional_encodings = embedded_positional_encodings.unsqueeze(2).expand(-1, -1, 27, -1)\n",
        "    # embedded_age = embedded_age.unsqueeze(1).unsqueeze(1).expand(-1, 13, 27, -1)\n",
        "    # embedded_race = embedded_race.unsqueeze(1).unsqueeze(1).expand(-1, 13, 27, -1)\n",
        "    # embedded_gender = embedded_gender.unsqueeze(1).unsqueeze(1).expand(-1, 13, 27, -1)\n",
        "    embedded_positional_encodings = embedded_positional_encodings.unsqueeze(2).expand(-1, -1, embedded_x.size(2), -1)\n",
        "    embedded_age = embedded_age.unsqueeze(1).unsqueeze(2).expand(-1, embedded_x.size(1), embedded_x.size(2), -1)\n",
        "    embedded_race = embedded_race.unsqueeze(1).unsqueeze(2).expand(-1, embedded_x.size(1), embedded_x.size(2), -1)\n",
        "    embedded_gender = embedded_gender.unsqueeze(1).unsqueeze(2).expand(-1, embedded_x.size(1), embedded_x.size(2), -1)\n",
        "\n",
        "    # print(\"Shape of embedded_x:\", embedded_x.shape)\n",
        "    # print(\"Shape of embedded_positional_encodings:\", embedded_positional_encodings.shape)\n",
        "    # print(\"Shape of embedded_age:\", embedded_age.shape)\n",
        "    # print(\"Shape of embedded_race:\", embedded_race.shape)\n",
        "    # print(\"Shape of embedded_gender:\", embedded_gender.shape)\n",
        "    embedded_input = torch.cat((embedded_x, embedded_positional_encodings, embedded_age, embedded_race, embedded_gender), dim=-1)\n",
        "    # HZ\n",
        "    embedded_input = self.embedding_projection(embedded_input)\n",
        "\n",
        "    print(embedded_input.shape)\n",
        "    # HZ\n",
        "    # reshaped_input = embedded_input.reshape(32, 13*27, -1)\n",
        "    reshaped_input = embedded_input.reshape(embedded_input.size(0), -1, self.projected_dim)\n",
        "    print(reshaped_input.shape)\n",
        "    # Apply transformer encoder\n",
        "    print(\"shape of masks: \", masks.shape)\n",
        "    # HZ\n",
        "    # reshaped_masks = masks.reshape(masks.size(0), -1)\n",
        "    if masks is not None:\n",
        "        reshaped_masks = masks.view(masks.size(0), -1)\n",
        "\n",
        "    # Confirm sizes match\n",
        "    print(f\"Input size: {embedded_input.shape}\")  # Expected [batch_size, seq_len, features]\n",
        "    print(f\"Mask size: {reshaped_masks.shape}\")   # Expected [batch_size, seq_len]\n",
        "\n",
        "    # encoder_output = self.transformer_encoder(reshaped_input, src_key_padding_mask=masks.reshape(32, 13*27))\n",
        "    encoder_output = self.transformer_encoder(reshaped_input, src_key_padding_mask= reshaped_masks)\n",
        "\n",
        "    # Apply transformer decoder\n",
        "    decoder_output = self.transformer_decoder(embedded_input, encoder_output, tgt_key_padding_mask=masks)\n",
        "\n",
        "    # Project decoder output to ICD code probabilities\n",
        "    logits = self.linear(decoder_output)\n",
        "\n",
        "    return logits\n",
        "\n",
        "# load the model here\n",
        "model = TranformEHR(num_gender_classes, num_race_classes, num_code=len(icd_codes_types), nhead=2, num_encoder_layers=1, num_decoder_layers=1)"
      ],
      "metadata": {
        "id": "a7GrbdiA-lGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training & Eveluation\n",
        "\n",
        "This code defines functions to train and evaluate a model using PyTorch for a task involving the TransformEHR architecture. Here's a breakdown of each part:\n",
        "\n",
        "#### Loss Function and Optimizer:\n",
        "\n",
        "* criterion = nn.CrossEntropyLoss(): Defines the cross-entropy loss function, commonly used for classification tasks.\n",
        "* optimizer = torch.optim.Adam(model.parameters()): Initializes the Adam optimizer to update the model parameters during training.\n",
        "\n",
        "#### Training Function (train):\n",
        "\n",
        "* Takes input model (the TransformEHR model), train_data_loader (dataloader for training data), and epochs (number of training epochs).\n",
        "* Sets the model to training mode (model.train()).\n",
        "* Iterates through each epoch and batch of data, computes the loss using the defined loss function, performs backpropagation, and updates the model parameters.\n",
        "* Optionally prints training progress.\n",
        "\n",
        "#### Evaluation Function (eval):\n",
        "\n",
        "* Takes input model (the TransformEHR model) and val_data_loader (dataloader for validation data).\n",
        "* Sets the model to evaluation mode (model.eval()).\n",
        "* Disables gradient calculation (torch.no_grad()) for efficiency during evaluation.\n",
        "* Computes the average loss on the validation data by iterating through batches and calculating the loss using the same criterion as in training.\n",
        "\n",
        "#### Example Usage:\n",
        "\n",
        "* Calls the train function to train the model for 2 epochs using the training data (train_loader).\n",
        "* Calls the eval function to evaluate the trained model using validation data (val_loader) and prints the average evaluation loss.\n",
        "\n",
        "Overall, this code snippet provides a structured way to train and evaluate a model using PyTorch, suitable for tasks like the TransformEHR architecture where data is fed in batches through dataloaders, and the model's performance is assessed using a loss function."
      ],
      "metadata": {
        "id": "9r4m_PpD21n8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define your loss function (e.g., cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define your optimizer (e.g., Adam)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "\n",
        "log_interval = 5\n",
        "\n",
        "def train(model, train_data_loader, epochs):\n",
        "  \"\"\"\n",
        "  Train the TransformEHR model on the provided dataloader.\n",
        "\n",
        "  Args:\n",
        "      model (nn.Module): The TransformEHR model to train.\n",
        "      dataloader (DataLoader): The dataloader containing training data.\n",
        "      epochs (int): Number of training epochs.\n",
        "  \"\"\"\n",
        "  model.train()  # Set model to training mode\n",
        "  for epoch in range(epochs):\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "      x, masks, gender, race, age, position_encodings = batch\n",
        "      # Move data to the device (GPU if available)\n",
        "      #x, masks, gender, race, age, position_encodings, labels = x.to(device), masks.to(device), gender.to(device), race.to(device), age.to(device), position_encodings.to(device), labels.to(device)\n",
        "      # Forward pass\n",
        "      logits = model(x, masks, gender, race, age, position_encodings)\n",
        "      loss = criterion(logits.view(-1, logits.size(-1)), x.view(-1))\n",
        "\n",
        "      # Backward pass and update parameters\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Print training progress (optional)\n",
        "      if (i + 1) % log_interval == 0:\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Step [{i+1}/{len(train_data_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "def eval(model, val_data_loader):\n",
        "  \"\"\"\n",
        "  Evaluate the TransformEHR model on the provided dataloader.\n",
        "\n",
        "  Args:\n",
        "      model (nn.Module): The TransformEHR model to evaluate.\n",
        "      dataloader (DataLoader): The dataloader containing evaluation data.\n",
        "\n",
        "  Returns:\n",
        "      float: Average loss on the evaluation data\n",
        "  \"\"\"\n",
        "  model.eval()  # Set model to evaluation mode\n",
        "  with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "    total_loss = 0\n",
        "    for x, masks, gender, race, age, position_encodings in val_data_loader:\n",
        "      # Move data to the device (GPU if available)\n",
        "      #x, masks, gender, race, age, position_encodings, labels = x.to(device), masks.to(device), gender.to(device), race.to(device), age.to(device), position_encodings.to(device), labels.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      logits = model(x, masks, gender, race, age, position_encodings)\n",
        "      loss = criterion(logits.view(-1, logits.size(-1)), x.view(-1))\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    # Calculate average loss\n",
        "    avg_loss = total_loss / len(val_data_loader)\n",
        "    return avg_loss\n",
        "\n",
        "# Example usage\n",
        "train(model, train_loader, 2)\n",
        "eval_loss = eval(model, val_loader)\n",
        "print(f\"Evaluation Loss: {eval_loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "MSTM749VPp69",
        "outputId": "d04734ea-acd2-4c92-f856-a061445e9c24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2220, 408, 16443, 3668, 889, 462, 1135, 75, 40, 12797, 3664, 283, 739, 403, 2145, 2462, 439], [409, 3627, 2746, 435, 22616, 889, 462, 75, 532, 40, 16443, 63, 283, 55, 739, 279], [408, 462, 3627, 2746, 429, 5235, 12342, 2462, 75, 40, 1491, 283, 10625, 55], [427, 20763, 390, 462, 1135, 889, 75, 40, 404, 3664, 12797, 63, 403, 2145, 283, 739, 55], [409, 2692, 16443, 889, 461, 732, 63, 283, 40, 75, 3664, 739, 55, 927, 398]] 1 0 84 ['2185-11-20', '2184-08-20', '2184-05-31', '2185-05-24', '2184-10-11']\n",
            "[[3712, 16388, 19684, 5158, 9731, 3010, 17851, 3814, 4354, 1161, 1437, 31921, 5955, 7372, 8632]] 0 0 70 ['2122-06-16']\n",
            "[[531, 4761], [3040, 666, 1595, 582, 11313, 229, 12582, 1175, 585, 751], [26203, 15609, 8591, 8824, 783, 2049, 22450, 1541, 75, 582, 7206, 156, 684], [383, 75, 4042, 721], [970, 971, 783, 1576, 75, 2012, 582, 4157, 1261], [75, 22465, 2820, 472, 3295], [516, 75, 9564, 7804]] 0 2 55 ['2169-07-22', '2167-08-01', '2170-06-02', '2169-02-13', '2170-06-29', '2167-11-09', '2168-02-03']\n",
            "[[7177, 113, 32891, 545, 9203, 903]] 0 8 23 ['2150-12-17']\n",
            "[[2662, 3814, 2247, 553]] 0 0 32 ['2135-06-27']\n",
            "[[7731, 1416, 4354, 22107], [20882, 16016, 395, 5211, 25461]] 0 0 33 ['2158-11-06', '2167-02-18']\n",
            "[[2122, 11859, 4093, 11756, 7036, 1125, 2885, 4361, 31807, 10453, 252, 13398, 4152, 6327, 531, 778, 7161, 8795, 20, 5033, 12905, 2546, 4193]] 0 0 75 ['2115-04-14']\n",
            "[[14193, 97], [24820, 6989]] 0 9 37 ['2184-06-24', '2187-01-10']\n",
            "[[559, 939, 204, 616, 7794, 512, 622, 55, 40, 538, 41, 927, 5791, 432, 3266]] 1 1 76 ['2144-02-10']\n",
            "[[1401, 75, 395, 214, 55, 3753, 222, 8114], [4732, 16743, 13382, 38337, 52913, 718, 49681, 4760, 75, 6928]] 0 1 57 ['2166-08-24', '2160-11-01']\n",
            "[[1076, 1488, 488, 75, 532, 801, 3116, 38442, 398, 52, 3013, 891, 6115], [10521, 17180, 2837, 628, 488], [1076, 38442, 1082, 488, 75, 801]] 1 21 50 ['2165-03-13', '2164-06-20', '2165-03-12']\n",
            "[[3259, 8265, 829, 9131, 1982, 1333, 1308, 3864, 414, 8929, 399, 3543, 3264]] 1 0 66 ['2164-05-10']\n",
            "[[13308, 75, 13819, 40, 14, 13726, 222, 3516]] 0 4 64 ['2152-08-21']\n",
            "[[56973, 252, 36041, 16830, 4875, 56974, 75, 4438, 6541, 56975, 8632, 1626, 11, 2507]] 1 4 68 ['2167-03-20']\n",
            "[[2055, 2496, 50]] 0 0 53 ['2196-12-10']\n",
            "[[2022, 14785, 674, 2616, 6397, 147, 4595, 1643, 3485, 8357, 5944, 664, 252, 260, 298, 250, 1441, 7301, 2876, 255, 296, 2302, 769, 1623, 11952, 2070, 474, 9670, 14867]] 0 0 80 ['2124-03-26']\n",
            "[[23004, 83, 24108, 2616, 9584, 2893, 24296, 230, 147]] 1 2 50 ['2156-05-11']\n",
            "[[55989, 4042, 4791, 516, 30877, 54518, 296, 75]] 0 17 65 ['2132-05-08']\n",
            "[[4947, 856, 9250, 3153, 633, 317, 224, 21156, 260, 10423, 1384, 2556, 1920, 16003, 2224, 91, 7612, 17750, 710, 1687, 12573, 15, 660, 240, 25222], [92, 92, 1680, 498, 231, 2651, 681, 292, 4006, 2546, 509, 777, 183, 8706, 3343, 15101, 11255, 6749, 9327, 648]] 1 0 79 ['2162-07-26', '2162-07-08']\n",
            "[[5833, 9469, 763, 160, 11756, 681, 2012, 786, 203, 5007, 236], [101, 23117, 292, 18478, 18900, 638, 1885, 1717, 871, 305, 174], [17804, 13482, 12215, 261, 75, 6331, 446, 183, 3845], [558, 420, 2039, 948, 556, 427, 3675, 622, 3699, 939, 3508, 2501, 730, 1294, 2203, 282, 1517, 2927, 561, 395, 801, 40, 572, 1298, 956, 2138, 571, 399, 1077, 570, 8114, 4837]] 0 0 72 ['2177-01-29', '2177-04-25', '2176-02-22', '2178-01-17']\n",
            "[[5471, 11800, 6729, 9548, 1705, 10980, 2494, 100, 2013, 5531, 3276, 154, 8653, 1671, 148], [901, 5379, 322, 14465, 5567, 154]] 1 0 52 ['2171-07-16', '2171-07-04']\n",
            "[[574, 252, 465, 40, 17893, 55, 1077, 739]] 1 0 87 ['2160-05-13']\n",
            "[[716, 664, 780, 305, 257, 174, 262, 1251, 8795, 6465], [498, 678, 1655, 1420, 1385, 4992, 711, 3563, 973, 637, 6903, 3160, 2391, 1415], [13715, 1204, 75, 255, 653, 981], [70325, 740, 15449, 4373, 147, 672, 6379, 229, 75]] 0 0 91 ['2165-08-29', '2166-03-21', '2162-05-15', '2164-01-29']\n",
            "[[65182, 4099, 57572, 12554, 10442, 22442], [10223, 18657, 12849]] 0 27 19 ['2173-08-04', '2170-09-30']\n",
            "[[61227, 14690, 34939, 32796, 2859, 2057, 2747, 1789, 2508, 6966, 24261, 1062, 1773, 20317, 811, 1982, 1488, 17786, 285, 1916, 2051]] 1 2 60 ['2110-04-01']\n",
            "[[4251, 5275, 22750, 4458, 1439, 127, 75, 1175, 130, 11156], [388, 12834, 75, 1782, 513, 577, 534, 4495, 214, 63, 414]] 0 0 23 ['2116-03-03', '2116-09-28']\n",
            "[[24702, 7167, 34634, 625, 52, 55, 214]] 0 0 43 ['2168-05-24']\n",
            "[[14082, 1514, 2967, 532, 211, 625, 75, 9988, 14157, 3742, 395, 536, 37556, 1820, 50, 54, 55, 877, 222, 2872, 8440, 5967, 534, 1061, 1297, 1775], [32260, 14530, 8792, 17650, 5280, 3007, 15543, 75, 671, 650, 9843, 10245, 20044, 19100, 21719], [10338, 586, 2723, 75, 11819, 8656, 1150, 16765, 922, 851, 29278], [16185, 5682, 57, 75, 488, 54, 536, 1785, 532, 219, 836, 50, 395, 2857, 55], [11236, 59373, 75, 582, 1964, 183, 861, 12672, 165, 13883], [1173, 75, 5403, 18308, 18355, 380, 1290, 30629, 1194, 6450, 37900], [42035, 252, 14243, 1626, 580, 75, 5676, 517, 1711, 851, 8565, 7942], [41887, 7605, 674, 4068, 791, 2759, 4150, 12630, 8239, 4521, 5891, 13094, 13551, 24289, 1438, 681, 1150, 43073, 851, 4137, 16767, 14358, 7273, 13233, 16920, 297, 118], [7211, 75, 905, 530, 1290, 3340, 40216]] 0 0 73 ['2171-01-09', '2166-03-01', '2165-08-19', '2174-01-14', '2166-07-25', '2166-03-28', '2166-02-08', '2168-11-15', '2166-02-15']\n",
            "[[714, 2441, 2443, 2326, 3397, 294, 1441, 1140, 527, 5446, 6519, 11669, 125, 9308, 14567, 3804, 6103, 3636, 17937, 658, 383, 5310]] 1 0 63 ['2172-11-27']\n",
            "[[33037, 16240, 7506, 4685, 3781, 904, 1057, 4678, 1805], [5670], [13366, 1947, 75, 183, 3701, 11144]] 0 2 69 ['2157-11-18', '2157-12-29', '2156-08-14']\n",
            "[[6469, 1514, 206, 214, 889, 697, 63, 282, 47, 425, 432, 440, 1789, 417, 1319, 1309, 738], [8573, 1514, 1562, 1294, 925, 1309, 29326, 412, 878, 889, 214, 439, 1789, 2860, 282, 697, 47], [2095, 4714, 2692, 2372, 646, 206, 408, 390, 889, 1789, 51, 9241, 1981, 282, 697, 47]] 0 0 74 ['2123-12-23', '2124-01-10', '2123-11-21']\n",
            "[[3054, 75, 229]] 0 0 66 ['2169-05-31']\n",
            "torch.Size([32, 9, 32, 128])\n",
            "torch.Size([32, 9, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 128])\n",
            "torch.Size([32, 9, 32, 128])\n",
            "torch.Size([32, 288, 128])\n",
            "shape of masks:  torch.Size([32, 9, 32])\n",
            "Input size: torch.Size([32, 9, 32, 128])\n",
            "Mask size: torch.Size([32, 288])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "expecting key_padding_mask shape of (288, 32), but got torch.Size([32, 288])",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a398d3fb4ced>\u001b[0m in \u001b[0;36m<cell line: 69>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Evaluation Loss: {eval_loss:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-a398d3fb4ced>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_data_loader, epochs)\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;31m#x, masks, gender, race, age, position_encodings, labels = x.to(device), masks.to(device), gender.to(device), race.to(device), age.to(device), position_encodings.to(device), labels.to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m       \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m       \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgender\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_encodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-257a5978c51f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, masks, gender, race, age, position_encodings)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;31m# encoder_output = self.transformer_encoder(reshaped_input, src_key_padding_mask=masks.reshape(32, 13*27))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreshaped_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mreshaped_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;31m# Apply transformer decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc_key_padding_mask_for_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert_to_nested\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_causal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    720\u001b[0m     def _sa_block(self, x: Tensor,\n\u001b[1;32m    721\u001b[0m                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n\u001b[0;32m--> 722\u001b[0;31m         x = self.self_attn(x, x, x,\n\u001b[0m\u001b[1;32m    723\u001b[0m                            \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                            \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1239\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1240\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1242\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5413\u001b[0m     \u001b[0;31m# merge key padding and attention masks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5414\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkey_padding_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5415\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5416\u001b[0m             \u001b[0;34mf\"expecting key_padding_mask shape of {(bsz, src_len)}, but got {key_padding_mask.shape}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5417\u001b[0m         \u001b[0mkey_padding_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: expecting key_padding_mask shape of (288, 32), but got torch.Size([32, 288])"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6bCcZNuxmz"
      },
      "source": [
        "# Results\n",
        "* We have completed data processing and feature engineering.\n",
        "* As part of data processing and analysis we have computed common and uncommon Disease/Outcome DOAP Dataset. DOAP dataset is display in table #1 and table#2 above.\n",
        "* MIMIC4 data has both ICD9CM and ICD10CM code. To have enough data for pretraining we have converted ICD9CM codes to ICD10CM codes. Since there is one to many relations between ICD9CM code and ICD10CM codes, we have randomly choosen any one ICD10CM code from all the possible ICD10CM code for ICD09 code.\n",
        "* To embade the time, we applied sinusoidal position embedding [2] to the numerical format of visit date (date-specific).\n",
        "* We have defined the model architecture and implemented TransformEHR model\n",
        "\n",
        "# Analyses\n",
        "* We have computed the data profiling using pyhealth MIMIC4 API. And splitted the data after computing the custom PyTorch dataset\n",
        " - Dataset: MIMIC4Dataset\n",
        "\t- Number of patients: 180733\n",
        "\t- Number of visits: 431231\n",
        "\t- Number of visits per patient: 2.3860\n",
        "\t- Number of events per visit in diagnoses_icd: 11.0296\n",
        "   - Length of train dataset: 144586\n",
        "   - Length of val dataset: 36147\n",
        "\n",
        "# Plans\n",
        "* Since the original code is not available, we are implementing the paper from scratch using PyHealth , PyTorch Transformer API. As of now we are working on Model Training and Eveluation component.\n",
        " - Next Action Items\n",
        "   - Complete pretrained model training and eveluation.\n",
        "   - Eveluate the pretrained model on DOAP datasets\n",
        "   - Implement Step#2 finetune on two diseases/outcomes.\n",
        "   - Prepare the eveluation figures\n",
        "   - If time permits, we will also work on model comparisons and ablations study.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjW9bCkouv8O"
      },
      "outputs": [],
      "source": [
        "# metrics to evaluate my model\n",
        "\n",
        "# plot figures to better show the results\n",
        "\n",
        "# it is better to save the numbers and figures for your presentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8EAWAy_LwHlV"
      },
      "source": [
        "## Model comparison - TBD\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOdhGrbwwG71"
      },
      "outputs": [],
      "source": [
        "#TBD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion - TBD\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2VDXo5F4Frm"
      },
      "outputs": [],
      "source": [
        "# no code is required for this section\n",
        "'''\n",
        "if you want to use an image outside this notebook for explanaition,\n",
        "you can read and plot it here like the Scope of Reproducibility\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "\n",
        "1. Yang, Z., Mitra, A., Liu, W. et al. TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records. Nat Commun 14, 7857 (2023). https://doi.org/10.1038/s41467-023-43715-z\n",
        "\n",
        "2. Vaswani, A. et al. Attention is All you Need. in Advances in Neural Information Processing Systems 30 (eds. Guyon, I. et al.) 5998–6008 (Curran Associates, Inc., 2017).https://arxiv.org/abs/1706.03762\n",
        "\n",
        "3. Rasmy, L., Xiang, Y., Xie, Z. et al. Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. npj Digit. Med. 4, 86 (2021). https://doi.org/10.1038/s41746-021-00455-y\n",
        "\n",
        "4. Li, Y., Rao, S., Solares, J.R.A. et al. BEHRT: Transformer for Electronic Health Records. Sci Rep 10, 7155 (2020). https://doi.org/10.1038/s41598-020-62922-y\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}