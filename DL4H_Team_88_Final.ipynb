{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlv6knX04FiY"
      },
      "source": [
        "# TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records.\n",
        "**CS598 Project**\n",
        "\n",
        "Anikesh Haran - anikesh2@illinois.edu         \n",
        "Satvik Kulkarni - satvikk2@illinois.edu         \n",
        "Changhua Zhan - zhan36@illinois.edu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Video Link\n",
        "\n",
        "https://mediaspace.illinois.edu/media/t/1_dih6uxuo"
      ],
      "metadata": {
        "id": "rMDlGAStlKzU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ0sNuMePBXx"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "The paper addresses the pressing need for accurate prediction of clinical diseases and outcomes using electronic health records (EHRs). Specifically, it focuses on the problem of disease prediction and outcome forecasting, which holds immense significance in enhancing patient care and healthcare management. This problem involves intricate feature engineering and data processing due to the complexity and interrelation of various diseases and outcomes. Additionally, the challenge lies in achieving high predictive accuracy amidst the vast and heterogeneous nature of EHR data. Traditional machine learning methods have been employed but are being outperformed by deep learning techniques.\n",
        "\n",
        "# State of the Art Methods\n",
        "The paper introduces TransformEHR, a novel denoising sequence to sequence transformer model, which tackles the limitations of existing methods. It innovatively pretrains on longitudinal EHRs to predict complete sets of ICD codes for future visits. The method's innovation lies in its generative encoder-decoder framework, which incorporates self-attention and cross-attention mechanisms. TransformEHR surpasses state-of-the-art BERT models, particularly excelling in predicting uncommon ICD codes.\n",
        "\n",
        "# TransformEHR\n",
        "The paper presents TransformEHR as a solution to the challenges in disease prediction and outcome forecasting. Its key innovation is the novel pretraining objective, which predicts all diseases and outcomes for future visits using longitudinal EHR data. Additionally, its generative encoder-decoder framework outperforms existing encoder-based models due to its attention mechanisms. TransformEHR achieves significant improvements in predicting both common and uncommon ICD codes, showcasing its effectiveness.\n",
        "\n",
        "General Problem\n",
        "\n",
        "Accurately predicting disease outcomes using electronic health records (EHRs) is crucial for preventative medicine, personalized healthcare and treatment planning. Traditional machine learning methods have achieved success in this domain, but recent advancements in deep learning offer the potential for even greater accuracy by harnessing the complexity and temporal dynamics of EHR data to enhance disease outcome prediction.\n",
        "\n",
        "Specific Approach\n",
        "\n",
        "This proposal aims to implement and evaluate the TransformEHR model, a transformer-based encoder-decoder generative model specifically designed for disease outcome prediction using EHRs. The model will be trained on a large dataset of anonymized patient records to learn meaningful representations and patterns associated with disease progression and outcomes.\n",
        "\n",
        "\n",
        "# Contribution to Research Regime\n",
        "The paper's contributions are multifaceted. Firstly, it proposes a new pretraining objective that captures complex interrelations among diseases and outcomes, addressing a critical gap in existing methods. Secondly, its innovative encoder-decoder framework sets a new standard for predictive modeling using EHRs, achieving superior performance compared to state-of-the-art methods. Thirdly, the study demonstrates the potential of TransformEHR in clinical screening and intervention, highlighting its practical significance. Overall, the paper significantly advances the field by offering a robust and effective solution to disease prediction and outcome forecasting using EHR data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uygL9tTPSVHB"
      },
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "The reproducibility scope entails implementing and evaluating the TransformEHR model, a transformer-based encoder-decoder generative model specifically designed for disease outcome prediction using Electronic Health Records (EHRs). The model will undergo training on the MIMIC-IV dataset, consisting of deidentified patient records. The objective is to validate the model's capacity to learn meaningful representations and patterns associated with disease progression and outcomes using the provided dataset.\n",
        "\n",
        "**Hypotheses**\n",
        "\n",
        "- TransformEHR will achieve competitive performance compared to traditional machine learning models in predicting various disease outcomes using EHR data.\n",
        "- The pre-training objective employed in TransformEHR, specifically predicting all future\n",
        "diagnoses, will improve the model's generalizability to diverse clinical prediction tasks.\n",
        "- The model will effectively capture temporal dependencies and complex patterns within EHR\n",
        "data, leading to more accurate predictions.\n",
        "- We will strive to distill complex patterns learned by TransformEHR into interpretable insights\n",
        "for clinicians, while achieving interpretability is inherently challenging in deep learning\n",
        "models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "**Pretrain-Finetune Paradigm**\n",
        "\n",
        "The Pretrain-Finetune paradigm is a widely used strategy in deep learning that involves two distinct phases to train a model effectively. In the pretraining phase, the model is trained on a large dataset using unsupervised or self-supervised learning tasks, such as language modeling or image reconstruction. This phase aims to capture general patterns and features from the data domain, leveraging the vast amount of information available in the large dataset. The pretrained model learns rich representations and general knowledge, which can be transferred to various downstream tasks.\n",
        "\n",
        "Following pretraining, the finetuning phase involves adapting the pretrained model to a specific task or domain by fine-tuning its parameters using a smaller, domain-specific dataset with labeled data. This dataset is typically more focused on the target task, such as classification or sequence labeling. By finetuning on this dataset, the model refines its learned representations to better suit the nuances and intricacies of the specific task. The combination of pretraining on a large dataset and finetuning on a smaller task-specific dataset allows the model to leverage both general knowledge and task-specific information, leading to improved performance and robustness on the target task.\n",
        "\n",
        "**Transform EHR**\n",
        "\n",
        "**Step #1** - first TransformEHR is pre-trained with a generative encoder-decoder transformer on a large set of EHR data. TransformEHR will learn the probability distribution of ICD codes against random distribution through the correlation of cross attention.\n",
        "\n",
        "**Step #2** - in the downstream finetuning, TransformEHR predicts a single disease or outcome. Through the calculated attention weights above, TransformEHR is able to identify top indicators for the predictions. This is shown in the picture below."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment\n",
        "\n",
        "###Platform & Notebook\n",
        "We have used Google Cloud Platform [GCP] - **Colab Enterprise Vertex AI Runtime** to reproduce this paper.\n",
        "\n",
        "#### Python Version\n",
        "* 3.10.12\n",
        "\n",
        "#### Packages & Dependencies\n",
        "* Pytorch - 2.2.1+cu121\n",
        "* Numpy - 1.25.2\n",
        "* PyHealth - 1.1.6\n",
        "\n",
        "#### Infrastrcture & Capacity\n",
        "\n",
        "* Colab Enterprise Vertex AI Runtime\n",
        "* Machine type - e2-highmem-16\n",
        "* CPU - 16\n",
        "* Memory - 128\n",
        "\n"
      ],
      "metadata": {
        "id": "xPqm90KxOyYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install required packages\n",
        "!pip install pyhealth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UZ7q7ZqpSvpO",
        "outputId": "21786e3a-ba81-47cb-ec5a-e19875ebdc81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyhealth\n",
            "  Downloading pyhealth-1.1.6-py2.py3-none-any.whl (311 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from pyhealth) (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from pyhealth) (0.17.1+cu121)\n",
            "Collecting rdkit>=2022.03.4 (from pyhealth)\n",
            "  Downloading rdkit-2023.9.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.24.2 in /usr/local/lib/python3.10/dist-packages (from pyhealth) (1.2.2)\n",
            "Requirement already satisfied: networkx>=2.6.3 in /usr/local/lib/python3.10/dist-packages (from pyhealth) (3.3)\n",
            "Collecting pandas<2,>=1.3.2 (from pyhealth)\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m82.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandarallel>=1.5.3 (from pyhealth)\n",
            "  Downloading pandarallel-1.6.5.tar.gz (14 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting mne>=1.0.3 (from pyhealth)\n",
            "  Downloading mne-1.7.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<=1.26.15 (from pyhealth)\n",
            "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyhealth) (1.25.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from pyhealth) (4.66.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from mne>=1.0.3->pyhealth) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from mne>=1.0.3->pyhealth) (3.1.3)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.10/dist-packages (from mne>=1.0.3->pyhealth) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from mne>=1.0.3->pyhealth) (3.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from mne>=1.0.3->pyhealth) (24.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.10/dist-packages (from mne>=1.0.3->pyhealth) (1.8.1)\n",
            "Requirement already satisfied: scipy>=1.7.1 in /usr/local/lib/python3.10/dist-packages (from mne>=1.0.3->pyhealth) (1.11.4)\n",
            "Collecting dill>=0.3.1 (from pandarallel>=1.5.3->pyhealth)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from pandarallel>=1.5.3->pyhealth) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.3.2->pyhealth) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2,>=1.3.2->pyhealth) (2023.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit>=2022.03.4->pyhealth) (9.4.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->pyhealth) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.2->pyhealth) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->pyhealth) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->pyhealth) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->pyhealth) (1.12)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->pyhealth) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->pyhealth)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->pyhealth)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->pyhealth)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->pyhealth)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->pyhealth)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->pyhealth)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->pyhealth)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->pyhealth)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->pyhealth)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.8.0->pyhealth)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->pyhealth)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->pyhealth) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->pyhealth)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne>=1.0.3->pyhealth) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne>=1.0.3->pyhealth) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne>=1.0.3->pyhealth) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne>=1.0.3->pyhealth) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.5.0->mne>=1.0.3->pyhealth) (3.1.2)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne>=1.0.3->pyhealth) (4.2.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.5->mne>=1.0.3->pyhealth) (2.31.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas<2,>=1.3.2->pyhealth) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->mne>=1.0.3->pyhealth) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->pyhealth) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.0.3->pyhealth) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.0.3->pyhealth) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.5->mne>=1.0.3->pyhealth) (2024.2.2)\n",
            "Building wheels for collected packages: pandarallel\n",
            "  Building wheel for pandarallel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandarallel: filename=pandarallel-1.6.5-py3-none-any.whl size=16673 sha256=7cbd07c3d476412f2c8120bb362cd2ef792f35ea8b09e9fea5924fe40117f92c\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/4f/1e/34e057bb868842209f1623f195b74fd7eda229308a7352d47f\n",
            "Successfully built pandarallel\n",
            "Installing collected packages: urllib3, rdkit, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, pandas, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pandarallel, nvidia-cusolver-cu12, mne, pyhealth\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dill-0.3.8 mne-1.7.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 pandarallel-1.6.5 pandas-1.5.3 pyhealth-1.1.6 rdkit-2023.9.6 urllib3-1.26.15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "import torch.nn.utils.rnn as rnn_utils"
      ],
      "metadata": {
        "id": "hkJAjo0-Swg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NbPHUTMbkD3"
      },
      "source": [
        "##  Data\n",
        "\n",
        "The dataset we plan to use in this project is MIMIC-IV from https://physionet.org. The MIMIC-IV dataset includes intensive care unit patients admitted to the Beth Israel Deaconess Medical Center in Boston, Massachusetts, comprises deidentified patient records used for medical research and analysis. It encompasses a wide range of clinical data, including demographic information, vital signs, laboratory results, medications, procedures, and clinical notes. MIMIC-IV offers longitudinal Electronic Health Records (EHRs) from various healthcare facilities, providing a comprehensive view of patient health trajectories. This dataset serves as a valuable resource for studying disease progression, treatment outcomes, predictive modeling, and other healthcare-related research endeavors.\n",
        "\n",
        "Since the dataset contains information from 2008 to 2019 but the implementation of ICD-10CM started from October 2015, to mimic the same dataset as per the paper, we have converted ICD9CM codes into ICD10CM codes first to have enough patients and visits for the cohorts for pretraining, resulting in a dataset of 180733 patients.\n",
        "\n",
        "**Longitudnal EHR Data**\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1hqh-LCYG6wGxSgyyoih_b5ox7SfxBzg-)\n",
        "\n",
        "**Data Includes**\n",
        "\n",
        "* Raw Data - MIMIC IV tables\n",
        "  * Admissions,\n",
        "  * Patient and\n",
        "  * Icd_diagonosis codes\n",
        "\n",
        "* Descriptive Statistics\n",
        "\t- Dataset: MIMIC4Dataset\n",
        "\t- Number of patients: 180733\n",
        "\t- Number of visits: 431231\n",
        "\t- Number of visits per patient: 2.3860\n",
        "\t- Number of events per visit in diagnoses_icd: 11.0296\n",
        "  - Train and Valdiation set - TBD\n",
        "\n",
        "**Data Processing (feature engineering)**\n",
        "\n",
        "**MIMIC-IV Cohort**\n",
        "\n",
        "Our pretraining cohort comprises 180733 patients and 431231 admissions. As per the paper To evaluate pretrained models, we created two disease/outcome agnostic prediction (DOAP) datasets—one for common and one for uncommon diseases/outcomes. We selected 10 ICD-10CM codes with the highest prevalence (prevalence ratio >2%) in our pretraining cohort for our common disease/outcome DOAP dataset. As for the set of uncommon diseases/outcomes, we followed the FDA guidelines30 to randomly select 10 ICD-10CM codes with a prevalence ratio ranging from 0.04% to 0.05% in our pretraining cohort. The lists of common and uncommon diseases/outcomes are shown in Table 1.\n",
        "\n",
        "**Data Processing**\n",
        "\n",
        "For data pre-processing we have used PyHealth pyhealth.datasets.MIMIC4Dataset to process the unstructured raw data into a structured dataset object. See the implementation section below.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1cE_7Xbbp5NWFi-l8b2xs4fMURgOqNy-d)\n",
        "\n",
        "**Created Common & Uncommon DataSet**\n",
        "Extract Relevant Information: Extract the necessary information from the MIMIC-IV dataset, including patient records, diagnoses, and outcomes.\n",
        "\n",
        "* Identify Prevalent ICD-10CM Codes: Identify the prevalent ICD-10CM codes in the pretraining cohort. For the common disease/outcome DOAP dataset, select 10 ICD-10CM codes with the highest prevalence ratio (>2%) in the pretraining cohort.\n",
        "\n",
        "* Select Uncommon ICD-10CM Codes: Follow the FDA guidelines to randomly select 10 ICD-10CM codes with a prevalence ratio ranging from 0.04% to 0.05% in the pretraining cohort for the set of uncommon diseases/outcomes.\n",
        "\n",
        "* Create Common Disease/Outcome DOAP Dataset: Filter the patient records to include only those with the selected common ICD-10CM codes. This will form the common disease/outcome DOAP dataset.\n",
        "\n",
        "* Create Uncommon Disease/Outcome DOAP Dataset: Similarly, filter the patient records to include only those with the selected uncommon ICD-10CM codes. This will form the uncommon disease/outcome DOAP dataset.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=13eJz6spok4iTyYVkAA5dcJY17AwQf7PQ)\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1hs6G1u7rjXLUTMyLJ5MerLrs1g0WVQLc)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzVUQS0CHry0"
      },
      "source": [
        "**Load Data**\n",
        "\n",
        "The MIMIC-IV dataset, a valuable resource for healthcare research, comes with stringent data sharing restrictions designed to protect patient privacy and ensure ethical use. Access to MIMIC-IV necessitates signing a Data Use Agreement (DUA) with the MIT Laboratory for Computational Physiology, outlining terms such as authorized use, privacy protection measures, and attribution requirements.\n",
        "\n",
        "Since we are bound to not share the RAW data. We have pre-processed the raw data and created the pickle files for quick loading and model traninign. We have checked in the processed pickle files into GitHub under data folder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAW Data Processing**\n",
        "\n",
        "For initial raw data processing, we have used PyHealth MIMIC4Dataset"
      ],
      "metadata": {
        "id": "7OO0aCNvs4wm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZScZNbROw-N"
      },
      "outputs": [],
      "source": [
        "\"\"\"from pyhealth.datasets import MIMIC4Dataset\n",
        "\n",
        "# dir and function to load raw data\n",
        "root = '/content/drive/MyDrive/DLH/MIMIC4/CSV/'\n",
        "\n",
        "def load_raw_data(raw_data_dir):\n",
        "  # implement this function to load raw data to dataframe/numpy array/tensor\n",
        "  mimic4_ds = MIMIC4Dataset(\n",
        "    # Argument 1: It specifies the data folder root.\n",
        "    root=raw_data_dir,\n",
        "\n",
        "    # Argument 2: The users need to input a list of raw table names (e.g., DIAGNOSES_ICD.csv, PROCEDURES_ICD.csv).\n",
        "    tables=[\"diagnoses_icd\"],\n",
        "    # Argument 3: This argument input a dictionary (key is the source code\n",
        "    # vocabulary and value is the target code vocabulary .\n",
        "    # Default is empty dict, which means the original code will be used.\n",
        "    # We will use ICD10 codes.\n",
        "    code_mapping={}\n",
        "    )\n",
        "  return mimic4_ds\n",
        "\n",
        "mimic4_ds = load_raw_data(root)\n",
        "\n",
        "mimic4_ds.info()\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsiGlnvDZ3g6",
        "outputId": "41e5df69-fc2c-4596-a61f-68a027db12ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Statistics of base dataset (dev=False):\n",
            "\t- Dataset: MIMIC4Dataset\n",
            "\t- Number of patients: 180733\n",
            "\t- Number of visits: 431231\n",
            "\t- Number of visits per patient: 2.3860\n",
            "\t- Number of events per visit in diagnoses_icd: 11.0296\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['diagnoses_icd']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "\"\"\"# Statistics of the entire dataset.\n",
        "mimic4_ds.stat()\n",
        "\n",
        "# You can find the list of all available tables in this dataset as\n",
        "mimic4_ds.available_tables\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"#Save data object to drive for quick retrival\n",
        "import pickle\n",
        "\n",
        "# Assuming your data object is named 'data_object'\n",
        "mimic4_ds_object_path = '/content/drive/MyDrive/DLH/MIMIC4/PKL/mimic4_ds.pkl'\n",
        "\n",
        "# Save the data object to Google Drive\n",
        "with open(mimic4_ds_object_path, 'wb') as f:\n",
        "    pickle.dump(mimic4_ds, f)\"\"\""
      ],
      "metadata": {
        "id": "MpVIqMZCrJQ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load MIMIC4 data from google drive\n",
        "import pickle\n",
        "\n",
        "# Path to the saved data object\n",
        "data_object_path = '/content/drive/MyDrive/DLH/MIMIC4/PKL/mimic4_ds.pkl'\n",
        "\n",
        "# Load the data object from Google Drive\n",
        "with open(data_object_path, 'rb') as f:\n",
        "    mimic4_data = pickle.load(f)\n",
        "\n",
        "# Statistics of the entire dataset.\n",
        "mimic4_data.stat()\n",
        "\n",
        "# You can find the list of all available tables in this dataset as\n",
        "mimic4_data.available_tables"
      ],
      "metadata": {
        "id": "ISDV4zCosDOB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "152befa0-ca99-4aad-da69-e2608223efbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Statistics of base dataset (dev=False):\n",
            "\t- Dataset: MIMIC4Dataset\n",
            "\t- Number of patients: 180733\n",
            "\t- Number of visits: 431231\n",
            "\t- Number of visits per patient: 2.3860\n",
            "\t- Number of events per visit in diagnoses_icd: 11.0296\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['diagnoses_icd']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Data**\n"
      ],
      "metadata": {
        "id": "Pmfx3saqstrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# get patient dictionary\n",
        "patient_dict = mimic4_data.patients\n",
        "print(list(patient_dict.keys())[:10])\n",
        "\n",
        "# get the \"10000032\" patient\n",
        "patient = patient_dict[\"10000032\"]\n",
        "print(patient)\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-Vt91of77_4",
        "outputId": "b1fce938-2df7-4194-f883-844cfe134cba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['10000032', '10000068', '10000084', '10000108', '10000117', '10000248', '10000280', '10000560', '10000635', '10000719']\n",
            "Patient 10000032 with 4 visits:\n",
            "\t- Birth datetime: 2128-04-07 00:00:00\n",
            "\t- Death datetime: 2180-09-09 00:00:00\n",
            "\t- Gender: F\n",
            "\t- Ethnicity: WHITE\n",
            "\t- anchor_year_group: 2014 - 2016\n",
            "\t- Visit 22595853 from patient 10000032 with 8 events:\n",
            "\t\t- Encounter time: 2180-05-06 22:23:00\n",
            "\t\t- Discharge time: 2180-05-07 17:15:00\n",
            "\t\t- Discharge status: 0\n",
            "\t\t- Available tables: ['diagnoses_icd']\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: 5723\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: 78959\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: 5715\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: 07070\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: 496\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: 29680\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: 30981\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22595853:\n",
            "\t\t\t- Code: V1582\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t- Visit 22841357 from patient 10000032 with 8 events:\n",
            "\t\t- Encounter time: 2180-06-26 18:27:00\n",
            "\t\t- Discharge time: 2180-06-27 18:49:00\n",
            "\t\t- Discharge status: 0\n",
            "\t\t- Available tables: ['diagnoses_icd']\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: 07071\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: 78959\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: 2875\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: 2761\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: 496\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: 5715\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: V08\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 22841357:\n",
            "\t\t\t- Code: 3051\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t- Visit 25742920 from patient 10000032 with 10 events:\n",
            "\t\t- Encounter time: 2180-08-05 23:44:00\n",
            "\t\t- Discharge time: 2180-08-07 17:50:00\n",
            "\t\t- Discharge status: 0\n",
            "\t\t- Available tables: ['diagnoses_icd']\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 07054\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 78959\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: V462\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 5715\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 2767\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 2761\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 496\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: V08\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 3051\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 25742920:\n",
            "\t\t\t- Code: 78791\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t- Visit 29079034 from patient 10000032 with 13 events:\n",
            "\t\t- Encounter time: 2180-07-23 12:35:00\n",
            "\t\t- Discharge time: 2180-07-25 17:55:00\n",
            "\t\t- Discharge status: 0\n",
            "\t\t- Available tables: ['diagnoses_icd']\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 45829\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 07044\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 7994\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 2761\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 78959\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 2767\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 3051\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: V08\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: V4986\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: V462\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 496\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 29680\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n",
            "\t\t- Event from patient 10000032 visit 29079034:\n",
            "\t\t\t- Code: 5715\n",
            "\t\t\t- Table: diagnoses_icd\n",
            "\t\t\t- Vocabulary: ICD9CM\n",
            "\t\t\t- Timestamp: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Common and Uncommon disease/outcome agnostic prediction (DOAP) datasets.**"
      ],
      "metadata": {
        "id": "akNjth33t44L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import random\n",
        "from collections import Counter\n",
        "\n",
        "# Step 1: Calculate the prevalence of each ICD-10CM code\n",
        "icd_counter = Counter()\n",
        "\n",
        "total_patients = 180733\n",
        "\n",
        "for patient in mimic4_sample:\n",
        "    for icd_code in patient['icd_codes']:\n",
        "        icd_counter[icd_code] += 1\n",
        "\n",
        "# Step 2: Select top 10 ICD-10CM codes with highest prevalence ratio (>2%) for common dataset\n",
        "# common_icd_codes = [icd_code for icd_code, count in icd_counter.items() if (count / total_patients) > 0.02][:10]\n",
        "# NOTE: we need diseases with the top 10 prevalence ratio\n",
        "common_icd_codes = [icd_code for icd_code, count in icd_counter.most_common(10)]\n",
        "# check whether all selected diseases has a prevalence ratio of > 2%\n",
        "print(sum([count/total_patients > 0.02 for icd_code, count in icd_counter.items() if icd_code in common_icd_codes]))\n",
        "\n",
        "\n",
        "# Step 3: Randomly select 10 ICD-10CM codes with prevalence ratio ranging from 0.04% to 0.05% for uncommon dataset\n",
        "uncommon_icd_codes = [icd_code for icd_code, count in icd_counter.items() if 0.0004 <= (count / total_patients) <= 0.0005]\n",
        "random.shuffle(uncommon_icd_codes)\n",
        "uncommon_icd_codes = uncommon_icd_codes[:10]\n",
        "\n",
        "# Step 4: Filter patient records to create common and uncommon datasets\n",
        "common_disease_dataset = [patient for patient in mimic4_sample if any(icd in patient['icd_codes'] for icd in common_icd_codes)]\n",
        "uncommon_disease_dataset = [patient for patient in mimic4_sample if any(icd in patient['icd_codes'] for icd in uncommon_icd_codes)]\n",
        "\n",
        "# Print the selected ICD-10CM codes for common and uncommon datasets\n",
        "print(\"Selected Common ICD-10CM Codes:\", common_icd_codes)\n",
        "print(\"Selected Uncommon ICD-10CM Codes:\", uncommon_icd_codes)\n",
        "\n",
        "# Optionally, print the lengths of the resulting datasets\n",
        "print(\"Number of patients in Common Disease/Outcome DOAP Dataset:\", len(common_disease_dataset))\n",
        "print(\"Number of patients in Uncommon Disease/Outcome DOAP Dataset:\", len(uncommon_disease_dataset))\"\"\""
      ],
      "metadata": {
        "id": "X2gVo3MxDRqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The lists of common and uncommon diseases/outcomes are shown in Table 1 and Table 2 respectivly.\n",
        "\n",
        "**Table 1 - Common ICD-10CM Codes**"
      ],
      "metadata": {
        "id": "sLVqo3e5nIbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the data for the table\n",
        "common_outcomes = {\n",
        "    'ICD-10-CM Code': ['I10', 'E785', 'Z87891', 'K219', 'F329', 'I2510', 'F419', 'N179', 'Z794', 'Z7901'],\n",
        "    'Description': [\n",
        "        'Essential (primary) hypertension',\n",
        "        'Hyperlipidemia, unspecified',\n",
        "        'Personal history of nicotine dependence',\n",
        "        'Gastro-esophageal reflux disease without esophagitis',\n",
        "        'Major depressive disorder, unspecified',\n",
        "        'Atherosclerotic heart disease of native coronary artery without angina pectoris',\n",
        "        'Unspecified anxiety disorder',\n",
        "        'Chronic kidney disease, unspecified',\n",
        "        'Long-term (current) use of insulin',\n",
        "        'Long-term (current) use of opiate analgesic'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the data\n",
        "common_outcomes_df = pd.DataFrame(common_outcomes)\n",
        "\n",
        "# Display the DataFrame\n",
        "common_outcomes_df"
      ],
      "metadata": {
        "id": "ROocXt6AnAo1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "235c972b-03b2-410e-fa2c-e624429c6f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  ICD-10-CM Code                                        Description\n",
              "0            I10                   Essential (primary) hypertension\n",
              "1           E785                        Hyperlipidemia, unspecified\n",
              "2         Z87891            Personal history of nicotine dependence\n",
              "3           K219  Gastro-esophageal reflux disease without esoph...\n",
              "4           F329             Major depressive disorder, unspecified\n",
              "5          I2510  Atherosclerotic heart disease of native corona...\n",
              "6           F419                       Unspecified anxiety disorder\n",
              "7           N179                Chronic kidney disease, unspecified\n",
              "8           Z794                 Long-term (current) use of insulin\n",
              "9          Z7901        Long-term (current) use of opiate analgesic"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-81fd59e9-3bd4-4c86-ba24-60e3ba3b6e1f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ICD-10-CM Code</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I10</td>\n",
              "      <td>Essential (primary) hypertension</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>E785</td>\n",
              "      <td>Hyperlipidemia, unspecified</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Z87891</td>\n",
              "      <td>Personal history of nicotine dependence</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>K219</td>\n",
              "      <td>Gastro-esophageal reflux disease without esoph...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>F329</td>\n",
              "      <td>Major depressive disorder, unspecified</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I2510</td>\n",
              "      <td>Atherosclerotic heart disease of native corona...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>F419</td>\n",
              "      <td>Unspecified anxiety disorder</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>N179</td>\n",
              "      <td>Chronic kidney disease, unspecified</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Z794</td>\n",
              "      <td>Long-term (current) use of insulin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Z7901</td>\n",
              "      <td>Long-term (current) use of opiate analgesic</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81fd59e9-3bd4-4c86-ba24-60e3ba3b6e1f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-81fd59e9-3bd4-4c86-ba24-60e3ba3b6e1f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-81fd59e9-3bd4-4c86-ba24-60e3ba3b6e1f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1e8e7357-b69e-4737-90d4-7aac2e151646\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1e8e7357-b69e-4737-90d4-7aac2e151646')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1e8e7357-b69e-4737-90d4-7aac2e151646 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "common_outcomes_df",
              "summary": "{\n  \"name\": \"common_outcomes_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"ICD-10-CM Code\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Z794\",\n          \"E785\",\n          \"I2510\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Long-term (current) use of insulin\",\n          \"Hyperlipidemia, unspecified\",\n          \"Atherosclerotic heart disease of native coronary artery without angina pectoris\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Table 2 - Uncommon ICD-10CM Codes**"
      ],
      "metadata": {
        "id": "4KWlL4n4ny32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the data for the table\n",
        "uncommon_outcomes = {\n",
        "    'ICD-10-CM Code': ['N94.6', 'T47.1X5D', 'O30.033', 'I70234', 'I95.2', 'Z34.83', 'C8518', 'L89.891', 'D126', 'I201'],\n",
        "    'Description': [\n",
        "        'Dyspareunia, unspecified',\n",
        "        'Poisoning by antineoplastic and immunosuppressive drugs, accidental (unintentional), subsequent encounter',\n",
        "        'Triplet pregnancy, fetus 3',\n",
        "        'Atherosclerosis of native arteries of extremities with gangrene, bilateral legs',\n",
        "        'Hypotension, unspecified',\n",
        "        'Supervision of high-risk pregnancy with other poor reproductive or obstetric history',\n",
        "        'Diffuse large B-cell lymphoma, lymph nodes of axilla and upper limb',\n",
        "        'Pressure ulcer of other site, stage 1',\n",
        "        'Benign neoplasm of colon',\n",
        "        'Unstable angina'\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame from the additional data\n",
        "uncommon_outcomes_df = pd.DataFrame(uncommon_outcomes)\n",
        "\n",
        "# Display the DataFrame\n",
        "uncommon_outcomes_df"
      ],
      "metadata": {
        "id": "Zbw08zfsoKrB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "9a7abd17-7c22-4d24-8c79-08c4f9b10bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  ICD-10-CM Code                                        Description\n",
              "0          N94.6                           Dyspareunia, unspecified\n",
              "1       T47.1X5D  Poisoning by antineoplastic and immunosuppress...\n",
              "2        O30.033                         Triplet pregnancy, fetus 3\n",
              "3         I70234  Atherosclerosis of native arteries of extremit...\n",
              "4          I95.2                           Hypotension, unspecified\n",
              "5         Z34.83  Supervision of high-risk pregnancy with other ...\n",
              "6          C8518  Diffuse large B-cell lymphoma, lymph nodes of ...\n",
              "7        L89.891              Pressure ulcer of other site, stage 1\n",
              "8           D126                           Benign neoplasm of colon\n",
              "9           I201                                    Unstable angina"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0262dd64-1ebe-40cc-8fbd-0aa712fd813e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ICD-10-CM Code</th>\n",
              "      <th>Description</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>N94.6</td>\n",
              "      <td>Dyspareunia, unspecified</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>T47.1X5D</td>\n",
              "      <td>Poisoning by antineoplastic and immunosuppress...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>O30.033</td>\n",
              "      <td>Triplet pregnancy, fetus 3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I70234</td>\n",
              "      <td>Atherosclerosis of native arteries of extremit...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I95.2</td>\n",
              "      <td>Hypotension, unspecified</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Z34.83</td>\n",
              "      <td>Supervision of high-risk pregnancy with other ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>C8518</td>\n",
              "      <td>Diffuse large B-cell lymphoma, lymph nodes of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>L89.891</td>\n",
              "      <td>Pressure ulcer of other site, stage 1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>D126</td>\n",
              "      <td>Benign neoplasm of colon</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>I201</td>\n",
              "      <td>Unstable angina</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0262dd64-1ebe-40cc-8fbd-0aa712fd813e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0262dd64-1ebe-40cc-8fbd-0aa712fd813e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0262dd64-1ebe-40cc-8fbd-0aa712fd813e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c05d3d3e-2982-4831-a36e-a3b008a9275e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c05d3d3e-2982-4831-a36e-a3b008a9275e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c05d3d3e-2982-4831-a36e-a3b008a9275e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "uncommon_outcomes_df",
              "summary": "{\n  \"name\": \"uncommon_outcomes_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"ICD-10-CM Code\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"D126\",\n          \"T47.1X5D\",\n          \"Z34.83\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Description\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Benign neoplasm of colon\",\n          \"Poisoning by antineoplastic and immunosuppressive drugs, accidental (unintentional), subsequent encounter\",\n          \"Supervision of high-risk pregnancy with other poor reproductive or obstetric history\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Patient Age - PreProcessing**\n",
        "\n",
        "Current PyHealth based data processing does not compute age feature. hence we pre-processed the patient's age separately and created a pickle files for age feature for quick loading during model training."
      ],
      "metadata": {
        "id": "lAi97MBsuOXG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import csv\n",
        "\n",
        "# Define the path to the CSV file\n",
        "root = '/content/drive/MyDrive/DLH/MIMIC4/CSV/'\n",
        "patient_file_path = root + 'patients.csv'\n",
        "\n",
        "id2age = {}\n",
        "\n",
        "# read id and age from patients.csv and save it in a dictionary id2age\n",
        "def read_patient_age(file_path):\n",
        "    with open(file_path, mode='r') as file:\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)\n",
        "        for row in reader:\n",
        "            id2age[row[0]] = int(row[2])\n",
        "\n",
        "read_patient_age(patient_file_path)\"\"\"\n"
      ],
      "metadata": {
        "id": "zEko0w_z-KF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Pre-Processing - transform_ehr_mimic4_fn**\n",
        "\n",
        "We have developed function **transform_ehr_mimic4_fn** to process individual patients and create feautres such as visit level details, icd codes and patinet;s demographic details such as age, gender and race.\n",
        "\n",
        "To reduce the data complexity and need of high compute power, we have pre-processed the longitudnal EHR data and kept fixed length sequence of Visit & ICD-Codes.\n",
        "\n",
        "* Visit Length - 4 visits per patient\n",
        "* ICD Codes - 5 ICD codes per visit\n",
        "\n",
        "Patients with less then 4 visits and less then 5 ICD diagnosis-codes have been discarded from the pre-traning cohort.\n",
        "\n",
        "**Total Patients In Pre-Training Cohort - 23206**\n"
      ],
      "metadata": {
        "id": "4Cx3SNQfuu-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Compute sequenced data for learning embeddings\n",
        "\n",
        "from datetime import datetime\n",
        "from pyhealth.medcode import CrossMap\n",
        "import random\n",
        "# set the random seed\n",
        "random.seed(0)\n",
        "\n",
        "# load the mapping from ICD9CM to CCSCM\n",
        "mapping_icd9cm_ccscm = CrossMap.load(source_vocabulary=\"ICD9CM\", target_vocabulary=\"CCSCM\")\n",
        "# load the mapping from CCSCM to ICD10CM\n",
        "mapping_ccscm_icd10cm = CrossMap.load(source_vocabulary=\"CCSCM\", target_vocabulary=\"ICD10CM\")\n",
        "\n",
        "#Calculate Patient's Age\n",
        "def calculate_age(birth_date, death_date):\n",
        "  # Calculate age\n",
        "  age = death_date.year - birth_date.year - ((death_date.month, death_date.day) < (birth_date.month, birth_date.day))\n",
        "  return age\n",
        "\n",
        "types = {}\n",
        "gender2idx = {}\n",
        "race2idx = {}\n",
        "\n",
        "def transform_ehr_mimic4_fn(patient):\n",
        "    visit_idx = []\n",
        "    newPatient = []\n",
        "    age = 0\n",
        "    gender = 0\n",
        "    race = 0\n",
        "    visit_dates = []\n",
        "    #consider patient with 4 or more visits\n",
        "    keep_patient = True\n",
        "    if len(patient) >= 4:\n",
        "      for i in range(len(patient)):\n",
        "        #visit level details\n",
        "        visit_idx.append(1 + i)\n",
        "\n",
        "        visit = patient[i]\n",
        "        conditions = []\n",
        "        events = visit.get_event_list(table=\"diagnoses_icd\")\n",
        "        if(len(events) < 5):\n",
        "          continue\n",
        "        formatted_visit_date = visit.encounter_time.strftime(\"%Y-%m-%d\")\n",
        "        visit_dates.append(formatted_visit_date)\n",
        "\n",
        "        for event in events:\n",
        "          vocabulary = event.vocabulary\n",
        "          code = \"\"\n",
        "          if vocabulary == \"ICD9CM\":\n",
        "            # map from ICD9CM to CCSCM\n",
        "            ccscmCodes = mapping_icd9cm_ccscm.map(event.code)\n",
        "            # in the case where one ICD9CM code maps to multiple CCSCM codes, randomly select one\n",
        "            ccscmCode = random.choice(ccscmCodes)\n",
        "\n",
        "            # map from CCSCM to ICD10CM\n",
        "            icd10cmCodes = mapping_ccscm_icd10cm.map(ccscmCode)\n",
        "            # in the case where one CCSCM code maps to multiple ICD10CM codes, randomly select one\n",
        "            code = random.choice(icd10cmCodes)\n",
        "          else:\n",
        "            code = event.code\n",
        "\n",
        "          if code in types:\n",
        "            conditions.append(types[code])\n",
        "          else:\n",
        "            types[code] = len(types)\n",
        "            conditions.append(types[code])\n",
        "\n",
        "        # step 2: assemble the sample\n",
        "        # if conditions is not empty, add the sample\n",
        "        # if (conditions): # commented it out because len(visit_date) needs to be the same as len(newPatient)\n",
        "        newPatient.append(conditions)\n",
        "\n",
        "      if(len(conditions) >= 4):\n",
        "        #visits.append(visit_idx)\n",
        "        if len(newPatient) > 100:\n",
        "          print(patient.patient_id,)\n",
        "        #visit_dates.append(visit_date)\n",
        "        #age.append(patient.anchor_age)\n",
        "\n",
        "        # get age of patient using patient id and id2age dictionary\n",
        "        #age.append(id2age[patient.patient_id])\n",
        "        age = id2age[patient.patient_id]\n",
        "\n",
        "        p_gender = patient.gender\n",
        "        if p_gender in gender2idx:\n",
        "          #gender.append(gender2idx[p_gender])\n",
        "          gender = gender2idx[p_gender]\n",
        "        else:\n",
        "          gender2idx[p_gender] = len(gender2idx)\n",
        "          #gender.append(gender2idx[p_gender])\n",
        "          gender = gender2idx[p_gender]\n",
        "\n",
        "        p_ethnicity = patient.ethnicity\n",
        "        if p_ethnicity in race2idx:\n",
        "          #race.append(race2idx[p_ethnicity])\n",
        "          race = race2idx[p_ethnicity]\n",
        "        else:\n",
        "          race2idx[p_ethnicity] = len(race2idx)\n",
        "          #race.append(race2idx[p_ethnicity])\n",
        "          race = race2idx[p_ethnicity]\n",
        "    return newPatient, visit_idx, age, gender, race, visit_dates\"\"\""
      ],
      "metadata": {
        "id": "d9migu3ZD41n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# Pre-processing of MIMIC4 Data\n",
        "seqs = []\n",
        "all_visits = []\n",
        "all_age = []\n",
        "all_gender = []\n",
        "all_race = []\n",
        "all_visit_dates = []\n",
        "patient_dict = mimic4_data.patients\n",
        "for patient_id in mimic4_data.patients:\n",
        "  patient = patient_dict[patient_id]\n",
        "  seq, visit_numbers, age, gender, race, visit_dates = transform_ehr_mimic4_fn(patient)\n",
        "  if seq and len(seq) >=4:\n",
        "    seqs.append(seq)\n",
        "    all_visits.append(visit_numbers)\n",
        "    all_age.append(age)\n",
        "    all_gender.append(gender)\n",
        "    all_race.append(race)\n",
        "    all_visit_dates.append(visit_dates)\n",
        "\n",
        "print(seqs[0])\"\"\"\n",
        "#[[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [16, 17, 18, 19, 20, 20, 4, 21, 22, 23], [24, 25, 26, 27, 28, 29, 30, 14, 31, 32, 4, 33, 34]]"
      ],
      "metadata": {
        "id": "C4q5-vrkdXWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ordering Visits Based on Visit Dates**"
      ],
      "metadata": {
        "id": "WdL2v9bjeKpy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"# sort the visit_date and seqs based on the visit date\n",
        "from datetime import datetime\n",
        "sorted_seqs = []\n",
        "sorted_visit_dates = []\n",
        "for i in range(len(seqs)):\n",
        "    visit_date = all_visit_dates[i]\n",
        "    seq = seqs[i]\n",
        "    visit_date_seq_tuple = [(visit_date[j], seq[j]) for j in range(len(seq))]\n",
        "    visit_date_seq_tuple.sort(key=lambda x: datetime.strptime(x[0], \"%Y-%m-%d\"))\n",
        "\n",
        "    sorted_visit_dates.append([x[0] for x in visit_date_seq_tuple])\n",
        "    sorted_seqs.append([x[1] for x in visit_date_seq_tuple])\n",
        "\n",
        "seqs = sorted_seqs\n",
        "all_visit_dates = sorted_visit_dates\n",
        "print(seqs[0])\n",
        "print(all_visit_dates[0])\"\"\"\n",
        "\n",
        "#[[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [24, 25, 26, 27, 28, 29, 30, 14, 31, 32, 4, 33, 34], [16, 17, 18, 19, 20, 20, 4, 21, 22, 23]]\n",
        "#['2180-05-06', '2180-06-26', '2180-07-23', '2180-08-05']"
      ],
      "metadata": {
        "id": "UIn497vReH1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"print(len(seqs))\n",
        "print(len(all_visits))\n",
        "print(len(all_visit_dates))\n",
        "print(len(all_gender))\n",
        "print(len(all_race))\n",
        "print(len(all_age))\n",
        "print(len(types))\"\"\"\n",
        "\n",
        "#23206\n",
        "#23206\n",
        "#23206\n",
        "#23206\n",
        "#23206\n",
        "#23206\n",
        "#51730"
      ],
      "metadata": {
        "id": "mdthOuLkea5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Pickle**\n",
        "\n",
        "In below code section we have created pickle files for all the data features - sequences, visit dates, gender , race & age and stored into filesystem.\n",
        "\n",
        "Note - change the \"path\" according to your environment."
      ],
      "metadata": {
        "id": "YXXCCW20wZht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"import pickle\n",
        "\n",
        "mimic4_ds_seqs_path = '/content/seqs.pkl'\n",
        "mimic4_ds_visits_path = '/content/visits.pkl'\n",
        "mimic4_ds_visit_dates_path = '/content/dates.pkl'\n",
        "mimic4_ds_type_path = '/content/type.pkl'\n",
        "mimic4_ds_gender_path = '/content/gender.pkl'\n",
        "mimic4_ds_race_path = '/content/race.pkl'\n",
        "mimic4_ds_age_path = '/content/age.pkl'\n",
        "\n",
        "# Save the data object to Google Drive\n",
        "with open(mimic4_ds_seqs_path, 'wb') as f:\n",
        "    pickle.dump(seqs, f)\n",
        "\n",
        "with open(mimic4_ds_visits_path, 'wb') as f:\n",
        "    pickle.dump(all_visits, f)\n",
        "\n",
        "with open(mimic4_ds_visit_dates_path, 'wb') as f:\n",
        "    pickle.dump(all_visit_dates, f)\n",
        "\n",
        "with open(mimic4_ds_type_path, 'wb') as f:\n",
        "    pickle.dump(types, f)\n",
        "\n",
        "with open(mimic4_ds_gender_path, 'wb') as f:\n",
        "    pickle.dump(all_gender, f)\n",
        "\n",
        "with open(mimic4_ds_race_path, 'wb') as f:\n",
        "    pickle.dump(all_race, f)\n",
        "\n",
        "with open(mimic4_ds_age_path, 'wb') as f:\n",
        "    pickle.dump(all_age, f)\"\"\""
      ],
      "metadata": {
        "id": "rBL_g0JZ5yD5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data for Model Training**"
      ],
      "metadata": {
        "id": "W8VYPfaDxA-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load preprocessed data from google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zq90lAOKksCL",
        "outputId": "867a9838-93dc-4869-a015-2ed294128a6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load MIMIC4 data from google drive\n",
        "import pickle\n",
        "\n",
        "# Path to the saved data object\n",
        "\"\"\"mimic4_ds_seqs_path = '/content/seqs.pkl'\n",
        "mimic4_ds_visits_path = '/content/visits.pkl'\n",
        "mimic4_ds_visit_dates_path = '/content/dates.pkl'\n",
        "mimic4_ds_type_path = '/content/type.pkl'\n",
        "mimic4_ds_gender_path = '/content/gender.pkl'\n",
        "mimic4_ds_race_path = '/content/race.pkl'\n",
        "mimic4_ds_age_path = '/content/age.pkl'\"\"\"\n",
        "\n",
        "mimic4_ds_seqs_path = '/content/drive/MyDrive/DLH/MIMIC4/PKL/v3/seqs.pkl'\n",
        "mimic4_ds_visits_path = '/content/drive/MyDrive/DLH/MIMIC4/PKL/v3/visits.pkl'\n",
        "mimic4_ds_visit_dates_path = '/content/drive/MyDrive/DLH/MIMIC4/PKL/v3/dates.pkl'\n",
        "mimic4_ds_type_path = '/content/drive/MyDrive/DLH/MIMIC4/PKL/v3/type.pkl'\n",
        "mimic4_ds_gender_path = '/content/drive/MyDrive/DLH/MIMIC4/PKL/v3/gender.pkl'\n",
        "mimic4_ds_race_path = '/content/drive/MyDrive/DLH/MIMIC4/PKL/v3/race.pkl'\n",
        "mimic4_ds_age_path = '/content/drive/MyDrive/DLH/MIMIC4/PKL/v3/age.pkl'\n",
        "\n",
        "# Load the data object from Google Drive\n",
        "with open(mimic4_ds_seqs_path, 'rb') as f:\n",
        "    seqs = pickle.load(f)\n",
        "\n",
        "with open(mimic4_ds_visits_path, 'rb') as f:\n",
        "    visits = pickle.load(f)\n",
        "\n",
        "# Load the data object from Google Drive\n",
        "with open(mimic4_ds_visit_dates_path, 'rb') as f:\n",
        "    visit_dates = pickle.load(f)\n",
        "\n",
        "# Load the data object from Google Drive\n",
        "with open(mimic4_ds_type_path, 'rb') as f:\n",
        "    icd_codes_types = pickle.load(f)\n",
        "\n",
        "# Load the data object from Google Drive\n",
        "with open(mimic4_ds_gender_path, 'rb') as f:\n",
        "    gender = pickle.load(f)\n",
        "\n",
        "# Load the data object from Google Drive\n",
        "with open(mimic4_ds_race_path, 'rb') as f:\n",
        "    race = pickle.load(f)\n",
        "\n",
        "# Load the data object from Google Drive\n",
        "with open(mimic4_ds_age_path, 'rb') as f:\n",
        "    age = pickle.load(f)"
      ],
      "metadata": {
        "id": "TP5Vc5mDGuUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build The Dataset**\n",
        "\n",
        "First, we have implemented a custom dataset using PyTorch Dataset class, which will characterize the key features of the dataset we want to generate.\n",
        "\n",
        "We will use the sequences of diagnosis-codes, gender, age, race and visit-dates as input for pretraning."
      ],
      "metadata": {
        "id": "CKBc3MUxxXfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, seqs, visits, gender, race, age, visit_dates):\n",
        "    self.x = seqs\n",
        "    self.visit = visits\n",
        "    self.gender = gender\n",
        "    self.race = race\n",
        "    self.age = age\n",
        "    self.visit_dates = visit_dates\n",
        "\n",
        "  def __len__(self):\n",
        "    # your code here\n",
        "    return len(self.x)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    # Extract the sequence\n",
        "    sequence = self.x[index]\n",
        "    visits = self.visit[index]\n",
        "    gender = self.gender[index]\n",
        "    race = self.race[index]\n",
        "    age = self.age[index]\n",
        "    visit_dates = self.visit_dates[index]\n",
        "    # Return the pair (sequence, hf)\n",
        "    return (sequence, visits, gender, race, age, visit_dates)\n",
        "\n",
        "dataset = CustomDataset(seqs, visits, gender, race, age, visit_dates)"
      ],
      "metadata": {
        "id": "dibp-dQ5qBRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.__getitem__(0))\n",
        "\n",
        "#Output\n",
        "#([[0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10, 11, 12, 13, 14, 15], [24, 25, 26, 27, 28, 29, 30, 14, 31, 32, 4, 33, 34], [16, 17, 18, 19, 20, 20, 4, 21, 22, 23]], [1, 2, 3, 4], 0, 0, 52, ['2180-05-06', '2180-06-26', '2180-07-23', '2180-08-05'])\n"
      ],
      "metadata": {
        "id": "CRuDt1jZmqzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Sampler & Split Data Into Train and Validation Set**\n",
        "\n",
        "We have also created a data sampler to quickly sample the data to test the model training, shapes and evaluation steps."
      ],
      "metadata": {
        "id": "zVaWl1njhSZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Run on sample\n",
        "from torch.utils.data import Dataset, SubsetRandomSampler\n",
        "\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "# Define the size of the subset (20% of the dataset)\n",
        "#sample_size = 0.2 #Sampling only 20 % of the dataset for model tranining and validation\n",
        "sample_size = 1.0 #Sampling all of the dataset 100% for model tranining and validation\n",
        "\n",
        "subset_size = int(sample_size * len(dataset))\n",
        "\n",
        "# Create a random sampler to sample indices from the dataset\n",
        "indices = list(range(len(dataset)))\n",
        "np.random.shuffle(indices)  # Shuffle the indices randomly\n",
        "subset_indices = indices[:subset_size]  # Take the first subset_size indices\n",
        "\n",
        "# Create a SubsetRandomSampler using the subset indices\n",
        "subset_sampler = SubsetRandomSampler(subset_indices)\n",
        "\n",
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "#use subset data and split in 80/20 for train and vel\n",
        "# Split the subset indices into training and validation indices (80/20 split)\n",
        "split_index = int(0.8 * len(subset_indices))\n",
        "train_indices = subset_indices[:split_index]\n",
        "val_indices = subset_indices[split_index:]\n",
        "\n",
        "# Create SubsetRandomSamplers for training and validation sets\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "val_sampler = SubsetRandomSampler(val_indices)\n",
        "\n",
        "print(\"Length of train dataset:\", len(train_sampler))\n",
        "print(\"Length of val dataset:\", len(val_sampler))\n",
        "\n",
        "#Length of train dataset: 18564\n",
        "#Length of val dataset: 4642"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8Ws4soah4Tc",
        "outputId": "fe3da20c-ef47-4220-ee3e-18efd4347ecd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of train dataset: 18564\n",
            "Length of val dataset: 4642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split Data Into Train and Validation Set**\n",
        "\n",
        "Another utility to split the dataset into training and validation sets without sampling."
      ],
      "metadata": {
        "id": "OY-uRdGyuJ1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "\n",
        "split = int(len(dataset)*0.8)\n",
        "\n",
        "lengths = [split, len(dataset) - split]\n",
        "train_dataset, val_dataset = random_split(dataset, lengths)\n",
        "\n",
        "print(\"Length of train dataset:\", len(train_dataset))\n",
        "print(\"Length of val dataset:\", len(val_dataset))\n",
        "\n",
        "#Length of train dataset: 18564\n",
        "#Length of val dataset: 4642"
      ],
      "metadata": {
        "id": "7NZYwtzRtw2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loader & collate_fn Implementation**\n",
        "\n",
        "Within collate_fu we are computing positional encoding to embed the time, we applied sinusoidal position embedding [2] to the numerical format of visit date (date-specific)"
      ],
      "metadata": {
        "id": "fzQ4EJ1GyAZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Data Loader and Collate Function**"
      ],
      "metadata": {
        "id": "plCq0D_ZkFcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "def load_sample_data(dataset, sampler, batch_size, shuffle):\n",
        "    def collate_fn(data):\n",
        "        def get_position_encoding(position, d_model):\n",
        "            \"\"\"Calculates sinusoidal position encoding for a given position and embedding dimension.\"\"\"\n",
        "            pe = torch.zeros(d_model)\n",
        "            div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "            #print(\"position\", position)\n",
        "            #print(\"div_term\",div_term)\n",
        "            position *= 2 * math.pi\n",
        "            pe[0::2] = torch.sin(position * div_term)\n",
        "            pe[1::2] = torch.cos(position * div_term)\n",
        "            return pe.unsqueeze(0)\n",
        "\n",
        "        sequences, visits_ids, gender, race, age, visit_dates = zip(*data)\n",
        "        # Convert gender and race to tensors (optional)\n",
        "        if gender is not None:\n",
        "            gender = torch.tensor(gender, dtype=torch.long)\n",
        "        if race is not None:\n",
        "            race = torch.tensor(race, dtype=torch.long)\n",
        "        if age is not None:\n",
        "            age = torch.tensor(age, dtype=torch.long)\n",
        "\n",
        "        sequences = [patient[-4:] for patient in sequences]\n",
        "        visit_dates = [visit_date[-4:] for visit_date in visit_dates]\n",
        "        visits_ids = [visit_id[:4] for visit_id in visits_ids]\n",
        "\n",
        "        #positional encoding dim\n",
        "        d_model = 2\n",
        "\n",
        "        num_patients = len(sequences)\n",
        "        num_visits = [len(patient) for patient in sequences]\n",
        "        num_codes = [len(visit) for patient in sequences for visit in patient]\n",
        "        max_num_visits = max(num_visits)\n",
        "        #max_num_visits = 4\n",
        "        max_num_codes = 5\n",
        "        pad_value = 0\n",
        "\n",
        "        visit_numbers = rnn_utils.pad_sequence([torch.tensor(visit) for visit in visits_ids], batch_first=True,padding_value=0)\n",
        "        num_heads = 1\n",
        "        x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
        "        masks = torch.zeros((num_patients, num_heads, max_num_visits, max_num_codes), dtype=torch.bool)\n",
        "        attn_masks = torch.zeros((num_patients, max_num_visits, max_num_visits), dtype=torch.bool)\n",
        "        position_encodings = torch.zeros((num_patients, max_num_visits, d_model),dtype=torch.float)  # For position encoding\n",
        "\n",
        "        for i_patient, (patient, visit_date) in enumerate(zip(sequences, visit_dates)):\n",
        "            valid_visits = [visit for visit in patient if len(visit) > 4]\n",
        "            if len(valid_visits) >= max_num_visits:\n",
        "                for h in range(num_heads):\n",
        "                  for j_visit, visit in enumerate(valid_visits[:max_num_visits]):\n",
        "\n",
        "                      last_5_icd_codes = visit[-5:]\n",
        "\n",
        "                      x[i_patient, j_visit, :] = torch.tensor(last_5_icd_codes, dtype=torch.long)\n",
        "\n",
        "                      # Calculate the attention mask\n",
        "                      attn_mask_row = [1] * (j_visit + 1) + [0] * (max_num_visits - j_visit - 1)\n",
        "                      attn_masks[i_patient, j_visit] = torch.tensor(attn_mask_row, dtype=torch.bool)\n",
        "\n",
        "                      # Create mask for the visit (mask all ICD codes in the visit)\n",
        "                      masks[i_patient, h, j_visit, :len(last_5_icd_codes)] = True\n",
        "\n",
        "                      if j_visit == len(valid_visits)-1:  # Check if it's the last visit for the patient\n",
        "                          masks[i_patient, h, j_visit, :] = False\n",
        "\n",
        "                      # Calculate position encoding based on visit date (assuming YYYY-MM-DD format)\n",
        "                      year, month, day = map(int, visit_date[j_visit].split('-'))\n",
        "                      # You can customize the date processing logic based on your data format\n",
        "                      date_as_float = year + (month - 1) / 12 + day / (365.25 * 12)  # Approximate date as float\n",
        "                      position_encodings[i_patient, j_visit, :] = get_position_encoding(date_as_float, d_model)\n",
        "\n",
        "        return (x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings)\n",
        "\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn, sampler=sampler)"
      ],
      "metadata": {
        "id": "Tz9SntGruSkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = load_sample_data(dataset, train_sampler, batch_size=32, shuffle=True)\n",
        "val_loader = load_sample_data(dataset, val_sampler, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "WoMoOTfIlgln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**All Data Loader & Collate Function**"
      ],
      "metadata": {
        "id": "w58oHI1ZkvzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import math\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "\n",
        "def load_data(dataset, batch_size, shuffle):\n",
        "    def collate_fn(data):\n",
        "        def get_position_encoding(position, d_model):\n",
        "            \"\"\"Calculates sinusoidal position encoding for a given position and embedding dimension.\"\"\"\n",
        "            pe = torch.zeros(d_model)\n",
        "            div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "            #print(\"position\", position)\n",
        "            #print(\"div_term\",div_term)\n",
        "            position *= 2 * math.pi\n",
        "            pe[0::2] = torch.sin(position * div_term)\n",
        "            pe[1::2] = torch.cos(position * div_term)\n",
        "            return pe.unsqueeze(0)\n",
        "\n",
        "        sequences, visits_ids, gender, race, age, visit_dates = zip(*data)\n",
        "        # Convert gender and race to tensors (optional)\n",
        "        if gender is not None:\n",
        "            gender = torch.tensor(gender, dtype=torch.long)\n",
        "        if race is not None:\n",
        "            race = torch.tensor(race, dtype=torch.long)\n",
        "        if age is not None:\n",
        "            age = torch.tensor(age, dtype=torch.long)\n",
        "\n",
        "        sequences = [patient[-4:] for patient in sequences]\n",
        "        visit_dates = [visit_date[-4:] for visit_date in visit_dates]\n",
        "        visits_ids = [visit_id[:4] for visit_id in visits_ids]\n",
        "\n",
        "        #positional encoding dim\n",
        "        d_model = 2\n",
        "        num_patients = len(sequences)\n",
        "        num_visits = [len(patient) for patient in sequences]\n",
        "        num_codes = [len(visit) for patient in sequences for visit in patient]\n",
        "        max_num_visits = max(num_visits)\n",
        "        #max_num_visits = 4\n",
        "        max_num_codes = 5\n",
        "        pad_value = 0\n",
        "\n",
        "        visit_numbers = rnn_utils.pad_sequence([torch.tensor(visit) for visit in visits_ids], batch_first=True,padding_value=0)\n",
        "        num_heads = 1\n",
        "        x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
        "        masks = torch.zeros((num_patients, num_heads, max_num_visits, max_num_codes), dtype=torch.bool)\n",
        "        attn_masks = torch.zeros((num_patients, max_num_visits, max_num_visits), dtype=torch.bool)\n",
        "        position_encodings = torch.zeros((num_patients, max_num_visits, d_model),dtype=torch.float)  # For position encoding\n",
        "\n",
        "        for i_patient, (patient, visit_date) in enumerate(zip(sequences, visit_dates)):\n",
        "            valid_visits = [visit for visit in patient if len(visit) > 4]\n",
        "            #print(valid_visits)\n",
        "            if len(valid_visits) >= max_num_visits:\n",
        "                for h in range(num_heads):\n",
        "                  for j_visit, visit in enumerate(valid_visits[:max_num_visits]):\n",
        "                      last_5_icd_codes = visit[-5:]\n",
        "\n",
        "                      x[i_patient, j_visit, :] = torch.tensor(last_5_icd_codes, dtype=torch.long)\n",
        "\n",
        "                      attn_mask_row = [1] * (j_visit + 1) + [0] * (max_num_visits - j_visit - 1)\n",
        "                      attn_masks[i_patient, j_visit] = torch.tensor(attn_mask_row, dtype=torch.bool)\n",
        "\n",
        "                      masks[i_patient, h, j_visit, :len(last_5_icd_codes)] = True\n",
        "\n",
        "                      if j_visit == len(valid_visits)-1:  # Check if it's the last visit for the patient\n",
        "                          masks[i_patient, h, j_visit, :] = False\n",
        "\n",
        "                      # Calculate position encoding based on visit date (assuming YYYY-MM-DD format)\n",
        "                      year, month, day = map(int, visit_date[j_visit].split('-'))\n",
        "                      date_as_float = year + (month - 1) / 12 + day / (365.25 * 12)  # Approximate date as float\n",
        "                      position_encodings[i_patient, j_visit, :] = get_position_encoding(date_as_float, d_model)\n",
        "\n",
        "        return (x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings)\n",
        "\n",
        "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, collate_fn=collate_fn)"
      ],
      "metadata": {
        "id": "EWQIm3kck4fQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = load_data(train_dataset, batch_size = 32)\n",
        "val_loader = load_data(val_dataset,  batch_size = 32)"
      ],
      "metadata": {
        "id": "uxVz0qugltlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the loader and collate function implementation\n",
        "loader_iter = iter(train_loader)\n",
        "x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings = next(loader_iter)\n",
        "print(x, attn_masks , masks,  visit_numbers, gender, race, age, position_encodings)"
      ],
      "metadata": {
        "id": "9xc2-EJdpMru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check shapes for all the features\n",
        "print(\"x\", x.shape)\n",
        "print(\"masks\", masks.shape)\n",
        "print(\"visits\", visit_numbers.shape)\n",
        "print(\"gender\", gender.shape)\n",
        "print(\"race\", race.shape)\n",
        "print(\"age\", age.shape)\n",
        "print(\"position_encodings\", position_encodings.shape)"
      ],
      "metadata": {
        "id": "pW6QJ6pXPM7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "duVR38domsJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Citation\n",
        "Yang, Z., Mitra, A., Liu, W. et al. TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records. Nat Commun 14, 7857 (2023). https://doi.org/10.1038/s41467-023-43715-z"
      ],
      "metadata": {
        "id": "oaPP00m6mzUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Link to the original paper's repo\n",
        "* TransformEHR model pre-training codebase is not available.\n",
        "* Finetuning Code Repo - https://github.com/whaleloops/TransformEHR\n",
        "\n",
        "Since pre-tranining codebase was not available, it took lot of research for the implementation. Hence we have implemented only the pre-training part of the paper.\n",
        "\n"
      ],
      "metadata": {
        "id": "eNiW8yf8m1Wh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TransformEHR Model Architecture\n",
        "\n",
        "TransformEHR uses a encoder-decoder architecture. The encoder takes in visit, time, and code/demographic embeddings and generates a set of hidden representations for each predictor. TransformEHR then calculates cross-attention over the encoder's created hidden representation. From there, these weighted representations are sent into the decoder, which then creates the ICD codes of the future visit. The decoder generates ICD codes in sequential order of code priority. so for example, we see a primary diagnosis and secondary diagnosis based on primary diagnosis. This process is continued until all diagnoses of a future visit are completed. This process is shown in the picture below.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1kyMUMOtLsFbM72MKnfe1tyMshlJiHZIn)\n",
        "\n",
        "### Pretraining Step\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1lW3i3PYLUlNgv8GoFXBHjQlZZq8dqDEL)\n",
        "\n",
        "### Finetuning Step\n",
        "![](https://drive.google.com/uc?export=view&id=1kbcUNFOookyk6ohFj1gjTolQ_-BgnpyP)\n"
      ],
      "metadata": {
        "id": "gXMBlEqqy9aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TransformEHR Model\n",
        "\n",
        "This implementation of the TranformEHR model is designed for processing electronic health record (EHR) data. Here's a summary of the key components and functionalities:\n",
        "\n",
        "####Embedding Layers:\n",
        "\n",
        "* Embedding layers are used for categorical features such as gender and race.\n",
        "* Continuous features like age and position encodings are also embedded using linear layers.\n",
        "* Visit embeddings are obtained using an embedding layer based on the number of diagnosis codes.\n",
        "\n",
        "####Concatenation of Embeddings:\n",
        "\n",
        "* All embeddings (gender, race, age, position encodings, and visit embeddings) are concatenated along the feature dimension.\n",
        "* The concatenated embeddings are projected to a lower-dimensional space using a linear layer (embedding_projection).\n",
        "\n",
        "####Transformer Encoder:\n",
        "\n",
        "* Utilizes a transformer encoder with specified parameters like the number of encoder layers (num_encoder_layers) and the number of attention heads (nhead).\n",
        "* The encoder processes the concatenated embeddings.\n",
        "\n",
        "####Transformer Decoder:\n",
        "\n",
        "* Employs a transformer decoder with parameters such as the number of decoder layers (num_decoder_layers) and attention heads (nhead).\n",
        "* Takes the encoder output and the concatenated embeddings as inputs, with masking applied as needed.\n",
        "\n",
        "####Linear Layer for Output:\n",
        "\n",
        "*A linear layer (linear) is used to project the decoder output to predict probabilities for ICD codes.\n",
        "\n",
        "####Forward Method:\n",
        "\n",
        "* The forward method takes input data (x), masks for padding (masks), as well as gender, race, age, and position encodings.\n",
        "* It performs the embedding, concatenation, projection, transformer encoding, decoding, and output projection steps.\n",
        "\n",
        "####Model Initialization:\n",
        "\n",
        "* The model is initialized with specified parameters such as the number of gender classes, race classes, and the maximum number of visits and diagnosis codes.\n",
        "\n",
        "Overall, this implementation encapsulates the key components of the TranformEHR model for processing EHR data with transformer-based encoder-decoder architecture and cross attentions."
      ],
      "metadata": {
        "id": "rtNbQcMp1TE0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model v1 - TransformEHR_M1**\n",
        "\n",
        "For better understanding of the model architecture, masking strategy and debugging, first we exprimented with pre-training the model on sequence of ICD-Codes only\n",
        "\n",
        "#### Features\n",
        "* sequence of icd_diagnosis codes.\n",
        "\n",
        "#### Model Parameters\n",
        "* Batch Size = 32\n",
        "* Learning Rate = 0.0001\n",
        "* Number of Head = 1\n",
        "* Encoder Layer = 1\n",
        "* Decoder Layer = 1\n",
        "* Epoc = 10\n",
        "* Batch First = True\n",
        "* Norm First = True"
      ],
      "metadata": {
        "id": "F1YLT5s_o_Tw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Implementation - TranformEHR_M1**\n"
      ],
      "metadata": {
        "id": "hqCB32OGtktf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of classes for each categorical feature\n",
        "num_gender_classes = 2\n",
        "num_race_classes = 33\n",
        "# Define the maximum number of visits and diagnosis codes\n",
        "#num_visits = [len(visit) for visit in visits]\n",
        "#max_num_visits = max(num_visits)\n",
        "\n",
        "max_num_visits = 4\n",
        "max_num_codes = 5\n",
        "\n",
        "def get_encoder_mask(batch_size, seq_length):\n",
        "    # Create a square matrix with ones in the lower triangle (including the diagonal)\n",
        "    mask = torch.tril(torch.ones(seq_length, seq_length))\n",
        "    # Expand to match the batch size\n",
        "    mask = mask.unsqueeze(0).unsqueeze(1).expand(batch_size, 1, seq_length, seq_length)\n",
        "    return mask\n",
        "\n",
        "class TranformEHR_M1(nn.Module):\n",
        "    def __init__(self, num_gender_classes, num_race_classes, num_visits, num_code, nhead, num_encoder_layers,\n",
        "                 num_decoder_layers, embedding_dim=128):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.concatenated_dim = embedding_dim * 1\n",
        "        self.projected_dim = embedding_dim\n",
        "        self.num_heads = nhead\n",
        "\n",
        "        # Define the embedding layers\n",
        "        self.visit_number_embedding = nn.Embedding(num_embeddings=num_visits, embedding_dim=embedding_dim)\n",
        "        self.gender_embedding = nn.Embedding(num_gender_classes, embedding_dim)\n",
        "        self.race_embedding = nn.Embedding(num_race_classes, embedding_dim)\n",
        "\n",
        "        # Define the embeddings for other continuous features (age, position_encodings)\n",
        "        self.age_embedding = nn.Linear(1, embedding_dim)  #age is a continuous feature\n",
        "        self.position_encodings_embedding = nn.Linear(2, embedding_dim)  # position_encodings has 2 dimensions\n",
        "\n",
        "        self.visit_embedding = nn.Embedding(num_embeddings=num_code, embedding_dim=embedding_dim)\n",
        "\n",
        "        self.embedding_projection = nn.Linear(self.concatenated_dim, self.projected_dim)\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, batch_first=True, norm_first=True)\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        # Transformer decoder\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=self.projected_dim, nhead=nhead, batch_first=True, norm_first=True)\n",
        "\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "        # Linear layer to project decoder output to ICD code probabilities\n",
        "        self.linear = nn.Linear(self.projected_dim, num_code)\n",
        "\n",
        "\n",
        "    def forward(self, x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings):\n",
        "\n",
        "        max_visit_number = self.visit_number_embedding.num_embeddings - 1  # Get the max allowed index\n",
        "        visit_numbers = torch.clamp(visit_numbers, 0, max_visit_number)  # Clamp values to valid range\n",
        "        embedded_visits_number = self.visit_number_embedding(visit_numbers)\n",
        "        embedded_gender = self.gender_embedding(gender)\n",
        "        embedded_race = self.race_embedding(race)\n",
        "        embedded_age = self.age_embedding(age.float().unsqueeze(-1))\n",
        "        embedded_positional_encodings = self.position_encodings_embedding(position_encodings.float())\n",
        "\n",
        "        embedded_x = self.visit_embedding(x)\n",
        "\n",
        "        \"\"\"embedded_positional_encodings = embedded_positional_encodings.unsqueeze(2).expand(-1, -1, embedded_x.size(2),-1)\n",
        "        embedded_age = embedded_age.unsqueeze(1).unsqueeze(2).expand(-1, embedded_x.size(1), embedded_x.size(2), -1)\n",
        "        embedded_race = embedded_race.unsqueeze(1).unsqueeze(2).expand(-1, embedded_x.size(1), embedded_x.size(2), -1)\n",
        "        embedded_gender = embedded_gender.unsqueeze(1).unsqueeze(2).expand(-1, embedded_x.size(1), embedded_x.size(2),-1)\n",
        "        embedded_visits_number = embedded_visits_number.unsqueeze(2).expand(-1, embedded_x.size(1), embedded_x.size(2),-1)\"\"\"\n",
        "\n",
        "        embedded_input = embedded_x.reshape(embedded_x.size(0), -1, self.projected_dim)\n",
        "\n",
        "        #Compute the attn_mask\n",
        "        batch_size = x.size(0)\n",
        "        new_masks = get_encoder_mask(batch_size, 20)\n",
        "        new_masks = new_masks.reshape(batch_size*self.num_heads,20, 20)\n",
        "\n",
        "        #Apply Encoder\n",
        "        encoder_output = self.transformer_encoder(embedded_input, mask = new_masks)\n",
        "\n",
        "        # Apply transformer decoder\n",
        "        print(\"embedded_input.shape - \",embedded_input.shape)\n",
        "        print(\"encoder_output.shape - \",encoder_output.shape)\n",
        "        decoder_output = self.transformer_decoder(embedded_input, encoder_output, tgt_mask=new_masks)\n",
        "\n",
        "        # Calculate logits\n",
        "        logits = self.linear(decoder_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Instantiate the model_m1\n",
        "model_m1 = TranformEHR_M1(num_gender_classes, num_race_classes, num_visits=max_num_visits, num_code=len(icd_codes_types),nhead=1, num_encoder_layers=1, num_decoder_layers=1)\n"
      ],
      "metadata": {
        "id": "a7GrbdiA-lGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model v2 - TransformEHR_M2**\n",
        "\n",
        "In Model V2 - we have added more features -\n",
        "\n",
        "#### Features\n",
        "\n",
        "* Visit embeddings +\n",
        "* Time (Visit Date) embeddings +\n",
        "* Demographic embeddings (Age, Gender and Race) +\n",
        "* ICD-Code embeddings\n",
        "\n",
        "\n",
        "\n",
        "#### Model Parameters\n",
        "* Batch Size = 32\n",
        "* Learning Rate = 0.0001\n",
        "* Number of Head = 1\n",
        "* Encoder Layer = 1\n",
        "* Decoder Layer = 1\n",
        "* Epoc = 10\n",
        "* Batch First = True\n",
        "* Norm First = True"
      ],
      "metadata": {
        "id": "gq3LjPO3slBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Implementation - TranformEHR_M2**"
      ],
      "metadata": {
        "id": "X2XstcontvVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of classes for each categorical feature\n",
        "num_gender_classes = 2\n",
        "num_race_classes = 33\n",
        "# Define the maximum number of visits and diagnosis codes\n",
        "#num_visits = [len(visit) for visit in visits]\n",
        "#max_num_visits = max(num_visits)\n",
        "max_num_visits = 4\n",
        "max_num_codes = 5\n",
        "\n",
        "def get_encoder_mask(batch_size, seq_length):\n",
        "    # Create a square matrix with ones in the lower triangle (including the diagonal)\n",
        "    mask = torch.tril(torch.ones(seq_length, seq_length))\n",
        "    # Expand to match the batch size\n",
        "    mask = mask.unsqueeze(0).unsqueeze(1).expand(batch_size, 1, seq_length, seq_length)\n",
        "    return mask\n",
        "\n",
        "class TranformEHR_M2(nn.Module):\n",
        "    def __init__(self, num_gender_classes, num_race_classes, num_visits, num_code, nhead, num_encoder_layers,\n",
        "                 num_decoder_layers, embedding_dim=128):\n",
        "        super().__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.concatenated_dim = embedding_dim * 6\n",
        "        self.projected_dim = embedding_dim\n",
        "        self.num_heads = nhead\n",
        "\n",
        "        # Define the embedding layers\n",
        "        self.visit_number_embedding = nn.Embedding(num_embeddings=num_visits, embedding_dim=embedding_dim)\n",
        "        self.gender_embedding = nn.Embedding(num_gender_classes, embedding_dim)\n",
        "        self.race_embedding = nn.Embedding(num_race_classes, embedding_dim)\n",
        "\n",
        "        # Define the embeddings for other continuous features (age, position_encodings)\n",
        "        self.age_embedding = nn.Linear(1, embedding_dim)  # Assuming age is a continuous feature\n",
        "        self.position_encodings_embedding = nn.Linear(2, embedding_dim)  # Assuming position_encodings has 2 dimensions\n",
        "        self.visit_embedding = nn.Embedding(num_embeddings=num_code, embedding_dim=embedding_dim)\n",
        "\n",
        "        self.embedding_projection = nn.Linear(self.concatenated_dim, self.projected_dim)\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=nhead, batch_first=True, norm_first=True)\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        # Transformer decoder\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=self.projected_dim, nhead=nhead, batch_first=True, norm_first=True)\n",
        "\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        # Linear layer to project decoder output to ICD code probabilities\n",
        "        self.linear = nn.Linear(self.projected_dim, num_code)\n",
        "\n",
        "    def forward(self, x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings):\n",
        "\n",
        "        max_visit_number = self.visit_number_embedding.num_embeddings - 1  # Get the max allowed index\n",
        "        visit_numbers = torch.clamp(visit_numbers, 0, max_visit_number)  # Clamp values to valid range\n",
        "        embedded_visits_number = self.visit_number_embedding(visit_numbers)\n",
        "        embedded_gender = self.gender_embedding(gender)\n",
        "        embedded_race = self.race_embedding(race)\n",
        "        embedded_age = self.age_embedding(age.float().unsqueeze(-1))\n",
        "        embedded_positional_encodings = self.position_encodings_embedding(position_encodings.float())\n",
        "        embedded_x = self.visit_embedding(x)\n",
        "\n",
        "        # Concatenate all embeddings\n",
        "        embedded_positional_encodings = embedded_positional_encodings.unsqueeze(2).expand(-1, -1, embedded_x.size(2), -1)\n",
        "        embedded_age = embedded_age.unsqueeze(1).unsqueeze(2).expand(-1, embedded_x.size(1), embedded_x.size(2), -1)\n",
        "        embedded_race = embedded_race.unsqueeze(1).unsqueeze(2).expand(-1, embedded_x.size(1), embedded_x.size(2), -1)\n",
        "        embedded_gender = embedded_gender.unsqueeze(1).unsqueeze(2).expand(-1, embedded_x.size(1), embedded_x.size(2),-1)\n",
        "        embedded_visits_number = embedded_visits_number.unsqueeze(2).expand(-1, embedded_x.size(1), embedded_x.size(2),-1)\n",
        "\n",
        "        embedded_input = torch.cat((embedded_x, embedded_visits_number, embedded_positional_encodings, embedded_age,embedded_race, embedded_gender), dim=-1)\n",
        "\n",
        "        embedded_input = embedded_x.reshape(embedded_input.size(0), -1, self.projected_dim)\n",
        "\n",
        "        #Calculate attn_mask\n",
        "        batch_size = x.size(0)\n",
        "        new_masks = get_encoder_mask(batch_size, 20)\n",
        "        new_masks = new_masks.reshape(batch_size*self.num_heads,20, 20)\n",
        "\n",
        "        #Apply Transformer Encoder\n",
        "        encoder_output = self.transformer_encoder(embedded_input, mask = new_masks)\n",
        "\n",
        "        # Apply Transformer Decoder\n",
        "        print(\"embedded_input.shape - \",embedded_input.shape)\n",
        "        print(\"encoder_output.shape - \",encoder_output.shape)\n",
        "        decoder_output = self.transformer_decoder(embedded_input, encoder_output, tgt_mask=new_masks)\n",
        "\n",
        "        logits = self.linear(decoder_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Instantiate the model_m2\n",
        "model_m2 = TranformEHR_M2(num_gender_classes, num_race_classes, num_visits=max_num_visits, num_code=len(icd_codes_types),nhead=1, num_encoder_layers=1, num_decoder_layers=1)\n"
      ],
      "metadata": {
        "id": "8n22bNLLtbLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training & Eveluation\n",
        "\n",
        "This code defines functions to train and evaluate a model using PyTorch for a task involving the TransformEHR architecture. Here's a breakdown of each part:\n",
        "\n",
        "#### Loss Function and Optimizer:\n",
        "\n",
        "* criterion = nn.CrossEntropyLoss(): Defines the cross-entropy loss function, commonly used for classification tasks.\n",
        "* optimizer = torch.optim.Adam(model.parameters()): Initializes the Adam optimizer to update the model parameters during training.\n",
        "\n",
        "#### Training Function (train):\n",
        "\n",
        "* Takes input model (the TransformEHR model), train_data_loader (dataloader for training data), and epochs (number of training epochs).\n",
        "* Sets the model to training mode (model.train()).\n",
        "* Iterates through each epoch and batch of data, computes the loss using the defined loss function, performs backpropagation, and updates the model parameters.\n",
        "* Optionally prints training progress.\n",
        "\n",
        "#### Evaluation Function (eval):\n",
        "\n",
        "* Takes input model (the TransformEHR model) and val_data_loader (dataloader for validation data).\n",
        "* Sets the model to evaluation mode (model.eval()).\n",
        "* Disables gradient calculation (torch.no_grad()) for efficiency during evaluation.\n",
        "* Computes the average loss on the validation data by iterating through batches and calculating the loss using the same criterion as in training.\n",
        "\n",
        "#### Example Usage:\n",
        "\n",
        "* Calls the train function to train the model for 10 epochs using the training data (train_loader).\n",
        "* Calls the eval function to evaluate the trained model using validation data (val_loader) and prints the average evaluation loss.\n",
        "\n",
        "Overall, this code snippet provides a structured way to train and evaluate a model using PyTorch, suitable for tasks like the TransformEHR architecture where data is fed in batches through dataloaders, and the model's performance is assessed using a loss function."
      ],
      "metadata": {
        "id": "9r4m_PpD21n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperparams**\n",
        "* Batch Size = 32\n",
        "* Learning Rate = 0.0001\n",
        "* Hidden Size = 128\n",
        "\n",
        "**Other Parameters**\n",
        "* Number of Head = 1\n",
        "* Encoder Layer = 1\n",
        "* Decoder Layer = 1\n",
        "* Epochs = 10\n",
        "* Batch First = True\n",
        "* Norm First = True\n",
        "\n",
        "**Computational requirements**\n",
        "* Hardware -\n",
        " * Colab Enterprise Vertex AI Runtime\n",
        " * Machine type - e2-highmem-16\n",
        " * CPU - 16\n",
        " * Memory - 128\n",
        "* Average Runtime for each epoch = ~3 minute\n",
        "* Total Execution Time - ~27 minutes\n",
        "* GPU hrs used = Model was trained using CPU's\n",
        "* Training epochs = 10"
      ],
      "metadata": {
        "id": "e2YNJWu-vYWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training & Evaluation - Model M1**"
      ],
      "metadata": {
        "id": "9eTtLmhZxN4B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import precision_score, roc_auc_score, average_precision_score, f1_score, accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "#for mac - use mps\n",
        "#for all other - use cuda\n",
        "\n",
        "#device = torch.device(\"cuda\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_m1.to(device)\n",
        "\n",
        "# Define your loss function (e.g., cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define your optimizer (e.g., Adam)\n",
        "optimizer = torch.optim.Adam(model_m1.parameters(), lr=0.0001)\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "#optimizer = torch.optim.Adam(model.parameters(),lr=1e-4)\n",
        "\n",
        "log_interval = 5\n",
        "\n",
        "train_losses = []  # List of training losses over epochs\n",
        "eval_losses = []  # List of evaluation losses over epochs\n",
        "accuracies = []  # List of accuracies over epochs\n",
        "precisions = []  # List of precisions over epochs\n",
        "recalls = []  # List of recalls over epochs\n",
        "f1_scores = []  # List of F1-scores over epochs\n",
        "topk_accuracies = []  # List of top-k accuracies over epochs\n",
        "precisions_macro = []  # List of macro precisions over epochs\n",
        "precisions_weighted = []  # List of weighted precisions over epochs\n",
        "f1_scores_macro = []  # List of macro F1-scores over epochs\n",
        "f1_scores_weighted = []  # List of weighted F1-scores over epochs\n",
        "roc_auc_scores_R = []  # List of ROC AUC scores over epochs\n",
        "roc_auc_scores_O = []\n",
        "average_precision_scores = []  # List of average precision scores over epochs\n",
        "all_probabilities_scores = []\n",
        "\n",
        "\n",
        "def train_eval(model, train_data_loader, val_data_loader, epochs):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1} of {epochs}\")\n",
        "        model.train() # Set model to training mode\n",
        "        epoch_loss = 0.0\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            print(batch_idx)\n",
        "            x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings = [item.to(device) for item in batch]\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), x.view(-1))\n",
        "\n",
        "            # Backward pass and update parameters\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        # Calculate average epoch loss\n",
        "        epoch_loss /= len(train_data_loader)\n",
        "        print(\"training_epoc_loss - \", epoch_loss)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        print(\"starting evaluation\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "            eval_epoch_loss = 0.0\n",
        "            all_predictions = []\n",
        "            all_probabilities = []\n",
        "            all_targets = []\n",
        "            for x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings in val_data_loader:\n",
        "                #x_batch_size = x.size(0)\n",
        "                #if not x_batch_size < 32:\n",
        "                x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings = x.to(device), attn_masks.to(device), masks.to(device), \\\n",
        "                visit_numbers.to(device), gender.to(device), race.to(device), age.to(device), \\\n",
        "                position_encodings.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                print(x.shape)\n",
        "                logits = model(x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings)\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), x.view(-1))\n",
        "                eval_epoch_loss += loss.item()\n",
        "\n",
        "                predicted_codes = torch.argmax(logits, dim=-1)\n",
        "                all_predictions.append(predicted_codes.cpu().numpy().flatten())\n",
        "                all_targets.append(x.cpu().numpy().flatten())\n",
        "\n",
        "                # Convert logits to probabilities\n",
        "                \"\"\"predicted_probs = torch.softmax(logits, dim=-1)\n",
        "                all_probabilities_scores.append(predicted_probs)\n",
        "                print(\"predicted_probs shape\", predicted_probs.shape)\n",
        "                all_probabilities.append(predicted_probs.cpu().numpy())\"\"\"\n",
        "\n",
        "            # Calculate average epoch loss\n",
        "            eval_epoch_loss /= len(val_data_loader)\n",
        "            eval_losses.append(eval_epoch_loss)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            all_predictions = np.concatenate(all_predictions)\n",
        "            \"\"\"all_probabilities_np = np.concatenate(all_probabilities, axis=0)\n",
        "            # Average the predicted probabilities across the sequence length dimension\n",
        "            average_probabilities = np.mean(all_probabilities_np, axis=1)\n",
        "            # Flatten the probabilities\n",
        "            flattened_probabilities = average_probabilities.reshape(-1)\"\"\"\n",
        "            all_targets = np.concatenate(all_targets)\n",
        "            accuracy = accuracy_score(all_targets, all_predictions)\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "            # Calculate precision, recall, and F1-score\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_predictions, average='macro')\n",
        "            precisions.append(precision)\n",
        "            recalls.append(recall)\n",
        "            f1_scores.append(f1)\n",
        "            # Compute evaluation metrics\n",
        "            precision_macro = precision_score(all_targets, all_predictions.round(), average='macro')\n",
        "            precision_weighted = precision_score(all_targets, all_predictions.round(), average='weighted')\n",
        "            precisions_macro.append(precision_macro)  # List of macro precisions over epochs\n",
        "            precisions_weighted.append(precision_weighted)  # List of weighted precisions over epochs\n",
        "\n",
        "            f1_score_macro = f1_score(all_targets, all_predictions.round(), average='macro')\n",
        "            f1_score_weighted = f1_score(all_targets, all_predictions.round(), average='weighted')\n",
        "            f1_scores_macro.append(f1_score_macro)  # List of macro F1-scores over epochs\n",
        "            f1_scores_weighted.append(f1_score_weighted)  # List of weighted F1-scores over epochs\n",
        "\n",
        "            #roc_auc_R = roc_auc_score(all_targets, flattened_probabilities, multi_class='ovr')\n",
        "            #roc_auc_O = roc_auc_score(all_targets, flattened_probabilities, multi_class='ovo')\n",
        "            #roc_auc_scores_R.append(roc_auc_R)  # List of ROC AUC scores over epochs\n",
        "            #roc_auc_scores_O.append(roc_auc_O)  # List of ROC AUC scores over epochs\n",
        "\n",
        "            #avg_precision = average_precision_score(all_targets, all_predictions) #default is - average='macro\n",
        "            #average_precision_scores.append(avg_precision)  # List of average precision scores over epochs\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Average Training Loss: {epoch_loss:.4f},\"\n",
        "              f\"Average Evaluation Loss : {eval_epoch_loss:.4f}, Accuracy: {accuracy:.4f},\"\n",
        "              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, \"\n",
        "              f\"Precision Macro: {precision_macro:.4f}, Precision Weighted: {precision_weighted:.4f}, \"\n",
        "              f\"F1-score Macro: {f1_score_macro:.4f}, F1-score Weighted: {f1_score_weighted:.4f}\")"
      ],
      "metadata": {
        "id": "o0mQernxxUxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train model_m1\n",
        "import time\n",
        "start = time.time()\n",
        "print(start)\n",
        "train_eval(model_m1, train_loader, val_loader, epochs=10)\n",
        "end = time.time()\n",
        "print(end)\n",
        "print(end - start)"
      ],
      "metadata": {
        "id": "mwm6mizqyK_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot evaluation metrics for Model M1**"
      ],
      "metadata": {
        "id": "chU4fHRmzKOh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_metrics(headline, train_losses, eval_losses, accuracies, precisions,\n",
        "                 recalls, f1_scores, precisions_macro, precisions_weighted,\n",
        "                 f1_scores_macro, f1_scores_weighted, roc_auc_scores_O,\n",
        "                 roc_auc_scores_R, average_precision_scores):\n",
        "    # Plot losses\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.suptitle(headline, fontsize=16)\n",
        "    # Plot losses\n",
        "    plt.figure(figsize=(10, 3))\n",
        "\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs, train_losses, label='Training Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs, eval_losses, label='Evaluation Loss', color='red')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Evaluation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot evaluation metrics\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs, accuracies, label='Accuracy')\n",
        "    plt.plot(epochs, precisions, label='Precision')\n",
        "    plt.plot(epochs, recalls, label='Recall')\n",
        "    plt.plot(epochs, f1_scores, label='F1-score')\n",
        "    #plt.plot(epochs, topk_accuracies, label='Top-k Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Metrics')\n",
        "    plt.title('Evaluation Metrics')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs, precisions_macro, label='Precisions Macro')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Precisions')\n",
        "    plt.title('Precisions [ Macro]')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs, precisions_weighted, label='Precisions Weighted')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Precisions')\n",
        "    plt.title('Precisions [ Weighted]')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs, f1_scores_macro, label='F1 Macro')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('F1-Score')\n",
        "    plt.title('F1-Score [ Macro]')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs, f1_scores_weighted, label='F1 Weighted')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('F1-Score')\n",
        "    plt.title('F1-Score [ Weighted]')\n",
        "    plt.legend()\n",
        "\n",
        "    \"\"\"#average_precision_scores\n",
        "    plt3.subplot(1, 3, 2)\n",
        "    plt3.plot(epochs, average_precision_scores, label='Average Precision')\n",
        "    plt3.xlabel('Epochs')\n",
        "    plt3.ylabel('Average Precision')\n",
        "    plt3.title('Average Precision')\n",
        "    plt3.legend()\n",
        "\n",
        "    plt3.subplot(1, 3, 3)\n",
        "    plt3.plot(epochs, roc_auc_scores_O, label='ROC-AUC-OVO')\n",
        "    plt3.xlabel('Epochs')\n",
        "    plt3.ylabel('ROC-AUC-OVO')\n",
        "    plt3.title('ROC-AUC [One-vs-one]')\n",
        "    plt3.legend()\n",
        "\n",
        "    plt.subplot(3, 3, 2)\n",
        "    plt.plot(epochs, roc_auc_scores_R, label='ROC-AUC-OVR')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('ROC-AUC-OVR')\n",
        "    plt.title('ROC-AUC [One-vs-rest]')\n",
        "    plt.legend()\"\"\"\n",
        "\n",
        "    #plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "7DgfX3LfyeHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headlind_1 = \"TransformEHR - Feature -[ICD Codes only], Num Head - 1, Encoder Layer - 1, Decoder Layer - 1, Learning Rate - 0.0001, Batch Size - 32\"\n",
        "plot_metrics(headlind_1, train_losses, eval_losses, accuracies, precisions, recalls,\n",
        "             f1_scores, precisions_macro, precisions_weighted, f1_scores_macro,\n",
        "             f1_scores_weighted, roc_auc_scores_O, roc_auc_scores_R, average_precision_scores)"
      ],
      "metadata": {
        "id": "S_lCHSMxzhPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create Check-Point for Model M1 for Finetuning**"
      ],
      "metadata": {
        "id": "IcJVM5MR1eWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a checkpoint dictionary:\n",
        "checkpoint = {\n",
        "    'model_state_dict': model_m1.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'epoch': 10,  # Current training epoch\n",
        "    'loss': train_losses   # Current training loss (optional)\n",
        "}\n",
        "\n",
        "filename = 'checkpoint_model1.pth'  # Create a unique filename\n",
        "torch.save(checkpoint, filename)"
      ],
      "metadata": {
        "id": "oZuysryq1cvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training & Evaluation - Model M2**"
      ],
      "metadata": {
        "id": "btLTDvkKzxZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import precision_score, roc_auc_score, average_precision_score, f1_score, accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "#for mac - use mps\n",
        "#for all other - use cuda\n",
        "\n",
        "#device = torch.device(\"cuda\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_m2.to(device)\n",
        "\n",
        "# Define your loss function (e.g., cross-entropy)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define your optimizer (e.g., Adam)\n",
        "optimizer = torch.optim.Adam(model_m2.parameters(), lr=0.0001)\n",
        "#optimizer = torch.optim.Adam(model_m2.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "#optimizer = torch.optim.Adam(model_m2.parameters(),lr=1e-4)\n",
        "\n",
        "log_interval = 5\n",
        "\n",
        "train_losses = []  # List of training losses over epochs\n",
        "eval_losses = []  # List of evaluation losses over epochs\n",
        "accuracies = []  # List of accuracies over epochs\n",
        "precisions = []  # List of precisions over epochs\n",
        "recalls = []  # List of recalls over epochs\n",
        "f1_scores = []  # List of F1-scores over epochs\n",
        "topk_accuracies = []  # List of top-k accuracies over epochs\n",
        "precisions_macro = []  # List of macro precisions over epochs\n",
        "precisions_weighted = []  # List of weighted precisions over epochs\n",
        "f1_scores_macro = []  # List of macro F1-scores over epochs\n",
        "f1_scores_weighted = []  # List of weighted F1-scores over epochs\n",
        "roc_auc_scores_R = []  # List of ROC AUC scores over epochs\n",
        "roc_auc_scores_O = []\n",
        "average_precision_scores = []  # List of average precision scores over epochs\n",
        "all_probabilities_scores = []\n",
        "\n",
        "\n",
        "def train_eval(model, train_data_loader, val_data_loader, epochs):\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1} of {epochs}\")\n",
        "        model.train() # Set model to training mode\n",
        "        epoch_loss = 0.0\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            print(batch_idx)\n",
        "            x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings = [item.to(device) for item in batch]\n",
        "\n",
        "            # Forward pass\n",
        "            logits = model(x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings)\n",
        "\n",
        "            loss = criterion(logits.view(-1, logits.size(-1)), x.view(-1))\n",
        "\n",
        "            # Backward pass and update parameters\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.item()\n",
        "        # Calculate average epoch loss\n",
        "        epoch_loss /= len(train_data_loader)\n",
        "        print(\"training_epoc_loss - \", epoch_loss)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        print(\"starting evaluation\")\n",
        "        model.eval()\n",
        "        with torch.no_grad():  # Disable gradient calculation for efficiency\n",
        "            eval_epoch_loss = 0.0\n",
        "            all_predictions = []\n",
        "            all_probabilities = []\n",
        "            all_targets = []\n",
        "            for x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings in val_data_loader:\n",
        "                #x_batch_size = x.size(0)\n",
        "                #if not x_batch_size < 32:\n",
        "                x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings = x.to(device), attn_masks.to(device), masks.to(device), \\\n",
        "                visit_numbers.to(device), gender.to(device), race.to(device), age.to(device), \\\n",
        "                position_encodings.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                print(x.shape)\n",
        "                logits = model(x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings)\n",
        "                loss = criterion(logits.view(-1, logits.size(-1)), x.view(-1))\n",
        "                eval_epoch_loss += loss.item()\n",
        "\n",
        "                predicted_codes = torch.argmax(logits, dim=-1)\n",
        "                all_predictions.append(predicted_codes.cpu().numpy().flatten())\n",
        "                all_targets.append(x.cpu().numpy().flatten())\n",
        "\n",
        "                # Convert logits to probabilities\n",
        "                \"\"\"predicted_probs = torch.softmax(logits, dim=-1)\n",
        "                all_probabilities_scores.append(predicted_probs)\n",
        "                print(\"predicted_probs shape\", predicted_probs.shape)\n",
        "                all_probabilities.append(predicted_probs.cpu().numpy())\"\"\"\n",
        "\n",
        "            # Calculate average epoch loss\n",
        "            eval_epoch_loss /= len(val_data_loader)\n",
        "            eval_losses.append(eval_epoch_loss)\n",
        "\n",
        "            # Calculate accuracy\n",
        "            all_predictions = np.concatenate(all_predictions)\n",
        "            \"\"\"all_probabilities_np = np.concatenate(all_probabilities, axis=0)\n",
        "            # Average the predicted probabilities across the sequence length dimension\n",
        "            average_probabilities = np.mean(all_probabilities_np, axis=1)\n",
        "            # Flatten the probabilities\n",
        "            flattened_probabilities = average_probabilities.reshape(-1)\"\"\"\n",
        "            all_targets = np.concatenate(all_targets)\n",
        "            accuracy = accuracy_score(all_targets, all_predictions)\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "            # Calculate precision, recall, and F1-score\n",
        "            precision, recall, f1, _ = precision_recall_fscore_support(all_targets, all_predictions, average='macro')\n",
        "            precisions.append(precision)\n",
        "            recalls.append(recall)\n",
        "            f1_scores.append(f1)\n",
        "            # Compute evaluation metrics\n",
        "            precision_macro = precision_score(all_targets, all_predictions.round(), average='macro')\n",
        "            precision_weighted = precision_score(all_targets, all_predictions.round(), average='weighted')\n",
        "            precisions_macro.append(precision_macro)  # List of macro precisions over epochs\n",
        "            precisions_weighted.append(precision_weighted)  # List of weighted precisions over epochs\n",
        "\n",
        "            f1_score_macro = f1_score(all_targets, all_predictions.round(), average='macro')\n",
        "            f1_score_weighted = f1_score(all_targets, all_predictions.round(), average='weighted')\n",
        "            f1_scores_macro.append(f1_score_macro)  # List of macro F1-scores over epochs\n",
        "            f1_scores_weighted.append(f1_score_weighted)  # List of weighted F1-scores over epochs\n",
        "\n",
        "            #roc_auc_R = roc_auc_score(all_targets, flattened_probabilities, multi_class='ovr')\n",
        "            #roc_auc_O = roc_auc_score(all_targets, flattened_probabilities, multi_class='ovo')\n",
        "            #roc_auc_scores_R.append(roc_auc_R)  # List of ROC AUC scores over epochs\n",
        "            #roc_auc_scores_O.append(roc_auc_O)  # List of ROC AUC scores over epochs\n",
        "\n",
        "            #avg_precision = average_precision_score(all_targets, all_predictions) #default is - average='macro\n",
        "            #average_precision_scores.append(avg_precision)  # List of average precision scores over epochs\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Average Training Loss: {epoch_loss:.4f},\"\n",
        "              f\"Average Evaluation Loss : {eval_epoch_loss:.4f}, Accuracy: {accuracy:.4f},\"\n",
        "              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}, \"\n",
        "              f\"Precision Macro: {precision_macro:.4f}, Precision Weighted: {precision_weighted:.4f}, \"\n",
        "              f\"F1-score Macro: {f1_score_macro:.4f}, F1-score Weighted: {f1_score_weighted:.4f}\")"
      ],
      "metadata": {
        "id": "mM52ia4uz4YN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train model_m2\n",
        "import time\n",
        "start = time.time()\n",
        "print(start)\n",
        "train_eval(model_m2, train_loader, val_loader, epochs=10)\n",
        "end = time.time()\n",
        "print(end)\n",
        "print(end - start)"
      ],
      "metadata": {
        "id": "rezkXnYm0atF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot evaluation metrics for Model M2**"
      ],
      "metadata": {
        "id": "ydaLVTHG0r5P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model #2\n",
        "headlind_2 = \"TransformEHR - Feature - all features], Num Head - 1, Encoder Layer - 1, Decoder Layer - 1, Learning Rate - 0.0001, Batch Size - 32\"\n",
        "plot_metrics(headlind_2, train_losses, eval_losses, accuracies, precisions, recalls,\n",
        "             f1_scores, precisions_macro, precisions_weighted, f1_scores_macro,\n",
        "             f1_scores_weighted, roc_auc_scores_O, roc_auc_scores_R, average_precision_scores)"
      ],
      "metadata": {
        "id": "ev9MUtBA0gt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create CheckPoints for Model M2 for Finetuning**"
      ],
      "metadata": {
        "id": "faOi2a_a1IxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a checkpoint dictionary:\n",
        "checkpoint = {\n",
        "    'model_state_dict': model_m2.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'epoch': 10,  # Current training epoch\n",
        "    'loss': train_losses   # Current training loss (optional)\n",
        "}\n",
        "\n",
        "filename = 'checkpoint_model2.pth'  # Create a unique filename\n",
        "torch.save(checkpoint, filename)"
      ],
      "metadata": {
        "id": "GbAqa-kO1UWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Disease/outcome agnostic prediction: AUROC scores on different pretraining objectives for the 10 common and 10 uncommon diseases**"
      ],
      "metadata": {
        "id": "tMS3ck3ihNui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_outcomes[\"ICD-10-CM Code ID\"] = [icd_codes_types[code] for code in common_outcomes[\"ICD-10-CM Code\"]]\n",
        "print(common_outcomes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWlWk45ghULJ",
        "outputId": "141447c9-cbc3-4fa0-b6d3-8bdf20d77c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ICD-10-CM Code': ['I10', 'E785', 'Z87891', 'K219', 'F329', 'I2510', 'F419', 'N179', 'Z794', 'Z7901'], 'Description': ['Essential (primary) hypertension', 'Hyperlipidemia, unspecified', 'Personal history of nicotine dependence', 'Gastro-esophageal reflux disease without esophagitis', 'Major depressive disorder, unspecified', 'Atherosclerotic heart disease of native coronary artery without angina pectoris', 'Unspecified anxiety disorder', 'Chronic kidney disease, unspecified', 'Long-term (current) use of insulin', 'Long-term (current) use of opiate analgesic'], 'ICD-10-CM Code ID': [233, 117, 186, 194, 403, 114, 315, 256, 126, 216]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uncommon_outcomes[\"ICD-10-CM Code ID\"] = [icd_codes_types[code] for code in uncommon_outcomes[\"ICD-10-CM Code\"]]\n",
        "print(uncommon_outcomes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y75cXkJdhZZN",
        "outputId": "898d1c35-7c94-4598-eb1f-33f15a96dca7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ICD-10-CM Code': ['N94.6', 'T47.1X5D', 'O30.033', 'I70234', 'I95.2', 'Z34.83', 'C8518', 'L89.891', 'D126', 'I201'], 'Description': ['Dyspareunia, unspecified', 'Poisoning by antineoplastic and immunosuppressive drugs, accidental (unintentional), subsequent encounter', 'Triplet pregnancy, fetus 3', 'Atherosclerosis of native arteries of extremities with gangrene, bilateral legs', 'Hypotension, unspecified', 'Supervision of high-risk pregnancy with other poor reproductive or obstetric history', 'Diffuse large B-cell lymphoma, lymph nodes of axilla and upper limb', 'Pressure ulcer of other site, stage 1', 'Benign neoplasm of colon', 'Unstable angina'], 'ICD-10-CM Code ID': [17717, 5915, 10067, 7927, 16772, 25471, 15870, 12703, 8390, 9204]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.metrics import precision_score, roc_auc_score, average_precision_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def evaluate_model(model, data_loader, outcomes):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  disease_codes = outcomes['ICD-10-CM Code ID']\n",
        "  num_diseases = len(disease_codes)\n",
        "  aurocs = []\n",
        "\n",
        "  predictions = {code: [] for code in disease_codes}\n",
        "  targets = {code: [] for code in disease_codes}\n",
        "\n",
        "  num = 0\n",
        "  with torch.no_grad():\n",
        "    for x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings in data_loader:\n",
        "      x = x.to(device)\n",
        "      # Run model\n",
        "      logits = model(x, attn_masks, masks, visit_numbers, gender, race, age, position_encodings)\n",
        "      probs = torch.sigmoid(logits[:, -1, :])  # Get the last visit probabilities\n",
        "\n",
        "      # Extract ICD codes for the last visit for each sample in the batch\n",
        "      last_visit_codes = x[:, -1, :]  # Shape: (batch_size, num_icd_codes_per_visit)\n",
        "\n",
        "      for i, code_id in enumerate(disease_codes):\n",
        "        # Check if each sample's last visit ICD codes contain the current disease code\n",
        "        code_mask = (last_visit_codes == code_id).any(dim=1)  # Shape: (batch_size)\n",
        "        batch_targets = code_mask.float().cpu().numpy()\n",
        "        # if (num == 0):\n",
        "        #   print(\"code_id:\", code_id)\n",
        "        #   print(\"last_visit_codes:\", last_visit_codes)\n",
        "        #   print(\"last_visit_codes shape:\", last_visit_codes.shape)\n",
        "        #   print(\"probs:\", probs)\n",
        "        #   print(\"probs shape:\", probs.shape)\n",
        "        #   print(\"batch_targets:\", batch_targets, batch_targets.shape)\n",
        "\n",
        "        # Collect targets and predictions\n",
        "        targets[code_id].extend(batch_targets)\n",
        "        predictions[code_id].extend(probs[:, i].cpu().numpy())\n",
        "      num += 1\n",
        "\n",
        "  # Calculate AUROC for each disease\n",
        "  for code_id in disease_codes:\n",
        "    if len(np.unique(targets[code_id])) > 1:\n",
        "      auroc = roc_auc_score(targets[code_id], predictions[code_id])\n",
        "      aurocs.append((outcomes['Description'][disease_codes.index(code_id)], auroc))\n",
        "\n",
        "  return aurocs"
      ],
      "metadata": {
        "id": "PaZo5_QMhaI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model M1 - AUROC scores on different pretraining objectives for the 10 common and 10 uncommon diseases**\n",
        "\n"
      ],
      "metadata": {
        "id": "2Ncd5XvAhu69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = load_data(dataset, batch_size = 32, shuffle=False)"
      ],
      "metadata": {
        "id": "YteDhlFyjqCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Model M1\n",
        "\n",
        "model_m1_checkpoint_path = '/content/drive/MyDrive/DLH/Resources/CheckPoint/checkpoint_may05-full-model1.pth'\n",
        "\n",
        "# Load model checkpoint\n",
        "model_m1_checkpoint = torch.load(model_m1_checkpoint_path)\n",
        "model_m1_state_dict = model_m1_checkpoint['model_state_dict']\n"
      ],
      "metadata": {
        "id": "fd-6JhVhiWsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_m1 = TranformEHR_M1(num_gender_classes, num_race_classes, num_visits=max_num_visits, num_code=len(icd_codes_types),nhead=1, num_encoder_layers=1, num_decoder_layers=1)\n",
        "\n",
        "# Load the model's state dictionary\n",
        "model_m1.load_state_dict(model_m1_state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_1f9tP8jJ18",
        "outputId": "bcd4a1eb-590d-4bfd-95db-5239cef55b34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute AUROC for 10 common diseases outcomes**\n",
        "\n"
      ],
      "metadata": {
        "id": "GXqvAVLFjyuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_m1_aurocs_common = evaluate_model(model_m1, data_loader, common_outcomes)"
      ],
      "metadata": {
        "id": "DInXdBtzhf_u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "df_results = pd.DataFrame(model_m1_aurocs_common, columns=['Disease', 'AUROC Score'])\n",
        "df_results['AUROC Score'] = df_results['AUROC Score'].round(2)\n",
        "display(df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "id": "XqnZ71Cuh3po",
        "outputId": "4898199b-8994-4b58-fb2d-c2a88d510d44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                             Disease  AUROC Score\n",
              "0                   Essential (primary) hypertension         0.53\n",
              "1                        Hyperlipidemia, unspecified         0.48\n",
              "2            Personal history of nicotine dependence         0.54\n",
              "3  Gastro-esophageal reflux disease without esoph...         0.47\n",
              "4             Major depressive disorder, unspecified         0.48\n",
              "5  Atherosclerotic heart disease of native corona...         0.57\n",
              "6                       Unspecified anxiety disorder         0.55\n",
              "7                Chronic kidney disease, unspecified         0.55\n",
              "8                 Long-term (current) use of insulin         0.39\n",
              "9        Long-term (current) use of opiate analgesic         0.37"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7218357d-84ee-4ef7-a47f-617deb265ef9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Disease</th>\n",
              "      <th>AUROC Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Essential (primary) hypertension</td>\n",
              "      <td>0.53</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hyperlipidemia, unspecified</td>\n",
              "      <td>0.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Personal history of nicotine dependence</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gastro-esophageal reflux disease without esoph...</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Major depressive disorder, unspecified</td>\n",
              "      <td>0.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Atherosclerotic heart disease of native corona...</td>\n",
              "      <td>0.57</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Unspecified anxiety disorder</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Chronic kidney disease, unspecified</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Long-term (current) use of insulin</td>\n",
              "      <td>0.39</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Long-term (current) use of opiate analgesic</td>\n",
              "      <td>0.37</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7218357d-84ee-4ef7-a47f-617deb265ef9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7218357d-84ee-4ef7-a47f-617deb265ef9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7218357d-84ee-4ef7-a47f-617deb265ef9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-9b82af4f-b0a7-48b9-9044-c3d287d261df\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-9b82af4f-b0a7-48b9-9044-c3d287d261df')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-9b82af4f-b0a7-48b9-9044-c3d287d261df button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Disease\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Long-term (current) use of insulin\",\n          \"Hyperlipidemia, unspecified\",\n          \"Atherosclerotic heart disease of native coronary artery without angina pectoris\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AUROC Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0688072993543886,\n        \"min\": 0.37,\n        \"max\": 0.57,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.48,\n          0.55,\n          0.53\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute AUROC for 10 uncommon diseases outcomes**"
      ],
      "metadata": {
        "id": "F-Laq8iqj6Jv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_m1_aurocs_uncommon = evaluate_model(model_m1, data_loader, uncommon_outcomes)"
      ],
      "metadata": {
        "id": "mZ8r2Vd1j55t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "df_results = pd.DataFrame(model_m1_aurocs_uncommon, columns=['Disease', 'AUROC Score'])\n",
        "df_results['AUROC Score'] = df_results['AUROC Score'].round(2)\n",
        "display(df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "rUDrHMXJkGJG",
        "outputId": "572f25b2-82ba-422e-c5e6-14e82fd233f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                             Disease  AUROC Score\n",
              "0                           Dyspareunia, unspecified         0.27\n",
              "1  Poisoning by antineoplastic and immunosuppress...         0.30\n",
              "2  Supervision of high-risk pregnancy with other ...         0.30\n",
              "3                           Benign neoplasm of colon         0.69\n",
              "4                                    Unstable angina         0.39"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3644a1cb-699b-477d-ac43-44747af02476\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Disease</th>\n",
              "      <th>AUROC Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dyspareunia, unspecified</td>\n",
              "      <td>0.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Poisoning by antineoplastic and immunosuppress...</td>\n",
              "      <td>0.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Supervision of high-risk pregnancy with other ...</td>\n",
              "      <td>0.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Benign neoplasm of colon</td>\n",
              "      <td>0.69</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Unstable angina</td>\n",
              "      <td>0.39</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3644a1cb-699b-477d-ac43-44747af02476')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-3644a1cb-699b-477d-ac43-44747af02476 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-3644a1cb-699b-477d-ac43-44747af02476');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-fc33c356-6520-4dbd-af30-56c82eca7216\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-fc33c356-6520-4dbd-af30-56c82eca7216')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-fc33c356-6520-4dbd-af30-56c82eca7216 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Disease\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Poisoning by antineoplastic and immunosuppressive drugs, accidental (unintentional), subsequent encounter\",\n          \"Unstable angina\",\n          \"Supervision of high-risk pregnancy with other poor reproductive or obstetric history\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AUROC Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17363755354185334,\n        \"min\": 0.27,\n        \"max\": 0.69,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.3,\n          0.39,\n          0.27\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model M2 - AUROC scores on different pretraining objectives for the 10 common and 10 uncommon diseases**"
      ],
      "metadata": {
        "id": "O1qkYON6wHaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Model M2\n",
        "\n",
        "model_m2_checkpoint_path = '/content/drive/MyDrive/DLH/Resources/CheckPoint/checkpoint_may05-full-model2.pth'\n",
        "\n",
        "# Load model checkpoint\n",
        "model_m2_checkpoint = torch.load(model_m2_checkpoint_path)\n",
        "model_m2_state_dict = model_m2_checkpoint['model_state_dict']"
      ],
      "metadata": {
        "id": "70rNrhpCwG-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_m2 = TranformEHR_M2(num_gender_classes, num_race_classes, num_visits=max_num_visits, num_code=len(icd_codes_types),nhead=1, num_encoder_layers=1, num_decoder_layers=1)\n",
        "\n",
        "# Load the model's state dictionary\n",
        "model_m2.load_state_dict(model_m2_state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kKi_Y4dwUzs",
        "outputId": "7519addf-faf8-47c4-c3ff-7f45a47b5e79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compute AUROC for 10 common diseases outcomes**"
      ],
      "metadata": {
        "id": "Hv2lRmC7xCzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_m2_aurocs_common = evaluate_model(model_m2, data_loader, common_outcomes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-V1m72mVxD9f",
        "outputId": "7e781ec7-e3c7-45ac-bde5-2c91291c997a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([6, 20, 128])\n",
            "encoder_output.shape -  torch.Size([6, 20, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "df_results = pd.DataFrame(model_m2_aurocs_common, columns=['Disease', 'AUROC Score'])\n",
        "df_results['AUROC Score'] = df_results['AUROC Score'].round(2)\n",
        "display(df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "_--dkWEmyj3s",
        "outputId": "0763db73-a455-49f0-f1f1-a24d7c770a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                             Disease  AUROC Score\n",
              "0                   Essential (primary) hypertension         0.52\n",
              "1                        Hyperlipidemia, unspecified         0.52\n",
              "2            Personal history of nicotine dependence         0.41\n",
              "3  Gastro-esophageal reflux disease without esoph...         0.58\n",
              "4             Major depressive disorder, unspecified         0.50\n",
              "5  Atherosclerotic heart disease of native corona...         0.49\n",
              "6                       Unspecified anxiety disorder         0.54\n",
              "7                Chronic kidney disease, unspecified         0.50\n",
              "8                 Long-term (current) use of insulin         0.41\n",
              "9        Long-term (current) use of opiate analgesic         0.60"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-122054af-1d0d-4b5f-9216-e0ac2df3bfdf\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Disease</th>\n",
              "      <th>AUROC Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Essential (primary) hypertension</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Hyperlipidemia, unspecified</td>\n",
              "      <td>0.52</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Personal history of nicotine dependence</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Gastro-esophageal reflux disease without esoph...</td>\n",
              "      <td>0.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Major depressive disorder, unspecified</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Atherosclerotic heart disease of native corona...</td>\n",
              "      <td>0.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Unspecified anxiety disorder</td>\n",
              "      <td>0.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Chronic kidney disease, unspecified</td>\n",
              "      <td>0.50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Long-term (current) use of insulin</td>\n",
              "      <td>0.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Long-term (current) use of opiate analgesic</td>\n",
              "      <td>0.60</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-122054af-1d0d-4b5f-9216-e0ac2df3bfdf')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-122054af-1d0d-4b5f-9216-e0ac2df3bfdf button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-122054af-1d0d-4b5f-9216-e0ac2df3bfdf');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-39045e0b-17ca-4792-9360-91e9744db258\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-39045e0b-17ca-4792-9360-91e9744db258')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-39045e0b-17ca-4792-9360-91e9744db258 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Disease\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Long-term (current) use of insulin\",\n          \"Hyperlipidemia, unspecified\",\n          \"Atherosclerotic heart disease of native coronary artery without angina pectoris\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AUROC Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06201254353399444,\n        \"min\": 0.41,\n        \"max\": 0.6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.52,\n          0.41,\n          0.54\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_m2_aurocs_uncommon = evaluate_model(model_m2, data_loader, uncommon_outcomes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "re47wGmoywF8",
        "outputId": "546b31f7-25c1-4ba2-c3e4-caaa0224d523"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([32, 20, 128])\n",
            "encoder_output.shape -  torch.Size([32, 20, 128])\n",
            "embedded_input.shape -  torch.Size([6, 20, 128])\n",
            "encoder_output.shape -  torch.Size([6, 20, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "df_results = pd.DataFrame(model_m2_aurocs_uncommon, columns=['Disease', 'AUROC Score'])\n",
        "df_results['AUROC Score'] = df_results['AUROC Score'].round(2)\n",
        "display(df_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "GGX4T3P_1RQ4",
        "outputId": "beef8f5a-86cf-4fc2-e319-d55297999dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                             Disease  AUROC Score\n",
              "0                           Dyspareunia, unspecified         0.84\n",
              "1  Poisoning by antineoplastic and immunosuppress...         0.83\n",
              "2  Supervision of high-risk pregnancy with other ...         0.71\n",
              "3                           Benign neoplasm of colon         0.59\n",
              "4                                    Unstable angina         0.43"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6217ad18-90db-4aae-a2d7-a790a46928c1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Disease</th>\n",
              "      <th>AUROC Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dyspareunia, unspecified</td>\n",
              "      <td>0.84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Poisoning by antineoplastic and immunosuppress...</td>\n",
              "      <td>0.83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Supervision of high-risk pregnancy with other ...</td>\n",
              "      <td>0.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Benign neoplasm of colon</td>\n",
              "      <td>0.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Unstable angina</td>\n",
              "      <td>0.43</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6217ad18-90db-4aae-a2d7-a790a46928c1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6217ad18-90db-4aae-a2d7-a790a46928c1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6217ad18-90db-4aae-a2d7-a790a46928c1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1671746b-50ad-4177-a384-376d0829d7f9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1671746b-50ad-4177-a384-376d0829d7f9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1671746b-50ad-4177-a384-376d0829d7f9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_results",
              "summary": "{\n  \"name\": \"df_results\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Disease\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Poisoning by antineoplastic and immunosuppressive drugs, accidental (unintentional), subsequent encounter\",\n          \"Unstable angina\",\n          \"Supervision of high-risk pregnancy with other poor reproductive or obstetric history\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AUROC Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17291616465790582,\n        \"min\": 0.43,\n        \"max\": 0.84,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          0.83,\n          0.43,\n          0.71\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX6bCcZNuxmz"
      },
      "source": [
        "# Results\n",
        "* We have completed data processing and feature engineering.\n",
        "* As part of data processing and analysis we have computed common and uncommon Disease/Outcome DOAP Dataset. DOAP dataset is display in table #1 and table#2 above.\n",
        "* MIMIC4 data has both ICD9CM and ICD10CM code. To have enough data for pretraining we have converted ICD9CM codes to ICD10CM codes. Since there is one to many relations between ICD9CM code and ICD10CM codes, we have randomly choosen any one ICD10CM code from all the possible ICD10CM code for ICD09 code.\n",
        "* To embade the time, we applied sinusoidal position embedding [2] to the numerical format of visit date (date-specific).\n",
        "* We have defined the model architecture and implemented TransformEHR model\n",
        "\n",
        "# Analyses\n",
        "* We have computed the data profiling using pyhealth MIMIC4 API. And splitted the data after computing the custom PyTorch dataset\n",
        " - Dataset: MIMIC4Dataset\n",
        "\t- Number of patients: 180733\n",
        "\t- Number of visits: 431231\n",
        "\t- Number of visits per patient: 2.3860\n",
        "\t- Number of events per visit in diagnoses_icd: 11.0296\n",
        "   - Length of train dataset: 144586\n",
        "   - Length of val dataset: 36147\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation Metrics - Model M1**\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1_OEvAbPhsra9Z4X6LzTHQqWXieCC8MmA)\n",
        "\n"
      ],
      "metadata": {
        "id": "x7MK_v362U7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrics - Model M2\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=10b3kMXI3PLq3PRsEeBUdps6ILSPch9Wo)"
      ],
      "metadata": {
        "id": "R43wEqhx2429"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Summary**"
      ],
      "metadata": {
        "id": "Ce4DtVh43daw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Define the data\n",
        "data = [\n",
        "    [\"Average Training Loss\", 0.0790, 0.0781],\n",
        "    [\"Average Evaluation Loss\", 0.4849, 0.4694],\n",
        "    [\"Accuracy\", 0.9720, 0.9726],\n",
        "    [\"Precision\", 0.7449, 0.7487],\n",
        "    [\"Recall\", 0.7678, 0.7710],\n",
        "    [\"F1-score\", 0.7527, 0.7562],\n",
        "    [\"Precision Macro\", 0.7449, 0.7487],\n",
        "    [\"Precision Weighted\", 0.9649, 0.9661],\n",
        "    [\"F1-score Macro\", 0.7527, 0.7562],\n",
        "    [\"F1-score Weighted\", 0.9677, 0.9685]\n",
        "]\n",
        "\n",
        "# Print the table\n",
        "print(tabulate(data, headers=[\"Metric\", \"Model-1 [Only ICD Codes] \", \"Model-2 [ICD Codes + Demographic Info + Visit Dates]\"]))"
      ],
      "metadata": {
        "id": "GEuMzTvX3fKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1GnnWZQBpPJAnnk2qt3FmXP97sloL_bGC)"
      ],
      "metadata": {
        "id": "BEotDrBI3g9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Disease/outcome agnostic prediction: AUROC scores on different pretraining objectives for the 10 common and 10 uncommon diseases**\n",
        "\n"
      ],
      "metadata": {
        "id": "H5G9EM8ihFlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results - Model M1 - AUROC scores on different pretraining objectives for the 10 common and 10 uncommon diseases**\n",
        "\n",
        "**AUROC Scores - Common Diseases**\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1XvMpt1-vSjceX61OjWQJ5L6KP2qhGtLK)"
      ],
      "metadata": {
        "id": "f2RaHwcih7cr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AUROC Scores - Uncommon Disease**\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Mj2ecVG3ESWPOkNwNBA7T5GewGXBzCUR)\n"
      ],
      "metadata": {
        "id": "0t1b94pSiI0j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results - Model M2 - AUROC scores on different pretraining objectives for the 10 common and 10 uncommon diseases**"
      ],
      "metadata": {
        "id": "75UG1fdTiG7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model-2 - AUROC Score - Common Disease**\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=11xvgy26x9Ge2rZxO6RKUotF8pphBDEtT)"
      ],
      "metadata": {
        "id": "-R_0cFxz1iAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model-2 - AUROC Score - Uncommon Disease**\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=14h5ExXdL9xrTcwFH5a0j1ziJKF9oJNL4)\n"
      ],
      "metadata": {
        "id": "HvCQbKOo2RGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparative Metrics Evaluation of TransformEHR Models\n",
        "This analysis compares the performance of two Pre-trainined TransformEHR models for predicting future visit ICD codes:\n",
        "\n",
        "Model 1: Trained on sequence of ICD codes (visit embedding) only.\n",
        "\n",
        "Model 2: Trained on visit embedding + demographic information (age, race, gender) and time embeddings [Visit Dates].\n",
        "\n",
        "The evaluation metrics suggest a slight advantage for Model 2:\n",
        "\n",
        "* Training Loss: Both models achieve similar training loss (~0.078).\n",
        "\n",
        "* Evaluation Loss: Model 2 has a lower average evaluation loss (0.4694) compared to Model 1 (0.4849).\n",
        "\n",
        "* Accuracy: Both models have very high accuracy (~0.97), making it difficult to distinguish between them based solely on this metric.\n",
        "\n",
        "* Precision: Model 2 shows slightly higher precision (0.7487) compared to Model 1 (0.7449). Precision indicates the proportion of correctly predicted positive cases.\n",
        "\n",
        "* Recall: Model 2 also has slightly higher recall (0.7710) compared to Model 1 (0.7678). Recall measures the proportion of actual positive cases that are correctly identified.\n",
        "\n",
        "* F1-score: Both metrics (Macro and Weighted) favor Model 2 with slightly higher F1-scores, which combines precision and recall.\n",
        "\n",
        "**Overall: While the differences are small, Model 2 appears to perform marginally better based on most metrics. The inclusion of demographic and time information seems to provide a slight advantage in predicting future ICD codes.**\n",
        "\n",
        "Note - The cohort used for training was fixed length vectors where we fixed number of visit to 4 and icd-codes to 5 per visit for simplicity."
      ],
      "metadata": {
        "id": "l-AaaLAD4vEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparative Metrics Evaluation of TransformEHR Models with AUROC Scores\n",
        "\n",
        "AUROC Scores:\n",
        "\n",
        "* Common Diseases: Both models achieved similar performance for most common diseases. However, Model 1 shows a slightly higher score for Atherosclerotic heart disease (0.576 vs 0.496).\n",
        "\n",
        "* Uncommon Diseases: Model 2 significantly outperforms Model 1 in all uncommon diseases, with a much larger improvement for Dyspareunia (0.271 vs 0.841) and Poisoning (0.302 vs 0.832).\n",
        "\n",
        "Overall Insights:\n",
        "\n",
        "* Demographic and Time Information: Including demographic and time data in Model 2 seems to improve the generalizability and robustness of the model, especially for uncommon diseases where visit history alone might be less informative.\n",
        "\n",
        "* Common vs. Uncommon Diseases: The impact of additional information is more pronounced for uncommon diseases. Demographic and temporal features can provide context that aids in predicting less frequent conditions.\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "**The AUROC scores solidify the findings from the previous analysis. While Model 1 performs adequately for common diseases, Model 2 demonstrates a clear advantage, particularly for uncommon diseases, due to the inclusion of demographic and time features. This highlights the importance of incorporating these additional factors for more comprehensive prediction of future patient health conditions.**"
      ],
      "metadata": {
        "id": "2PAxaalQ89kG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hypothesis & Results from the Original Paper\n",
        "\n",
        "**Hypothesis 1: Competitive Performance**\n",
        "\n",
        "Supported: The high accuracy (~0.97) suggests competitive performance. However, comparing AUROC scores with other models from the original paper would solidify this claim.\n",
        "\n",
        "**Hypothesis 2: Generalizability with Pre-training**\n",
        "\n",
        "Partially Supported: The model performs well for common diseases even without demographic and time information. However, the significant improvement for uncommon diseases in Model 2 suggests the pre-training objective targeting all diagnoses enhances generalizability.\n",
        "\n",
        "**Hypothesis 3: Capturing Temporal Dependencies**\n",
        "\n",
        "Supported: The inclusion of time embeddings in Model 2 demonstrates improved performance, suggesting the model captures temporal aspects of patient data for more accurate predictions.\n"
      ],
      "metadata": {
        "id": "JFfRA59F-VvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ablation Study\n",
        "\n",
        "Our TransformEHR contains three unique components compared to previous medical BERT-based models: (1) visit masking, (2) encoder-decoder architecture, and (3) time embedding.\n",
        "\n",
        "* Visit Masking: This involves masking all ICD codes within a visit during pre-training. This forces the model to learn relationships between codes within a visit and predict future codes based on the entire visit context.\n",
        "\n",
        "* Encoder-Decoder Architecture: Unlike standard BERT models which are encoders only, TransformEHR utilizes an encoder-decoder architecture. The encoder processes the past visit information, and the decoder generates predictions for future diagnoses. This allows for a more direct focus on predicting future outcomes.\n",
        "\n",
        "* Time Embedding: This component incorporates temporal information into the model by embedding visit dates. This helps the model capture the order and timing of past visits, potentially improving prediction accuracy, especially for diseases with time-sensitive aspects.\n",
        "\n",
        "**Ablation Analysis Setup:**\n",
        "\n",
        "We could not compare our model with other Encoder-Decoder implementation but We performed ablation analysis to evaluate the effectiveness of each component by training two models.\n",
        "\n",
        "* Model M1 (Baseline): This model only utilizes visit masking. It essentially represents the core functionality of TransformEHR without additional features.\n",
        "\n",
        "* Model M2 (Full Model): This model incorporates all three components - visit masking, encoder-decoder architecture, and time embedding. It represents the complete TransformEHR model as described.\n",
        "\n",
        "**Results:**\n",
        "\n",
        "By comparing the performance of M1 and M2 on various metrics (accuracy, AUROC etc.), our aim to assess the effectiveness of each additional component.\n",
        "\n",
        "It's expected that:\n",
        "\n",
        "* Model M2 (Full Model) will outperform M1 (Baseline) across most metrics, especially for uncommon diseases. This would demonstrate the benefit of incorporating demographic information and time embeddings for generalizability and robustness.\n",
        "\n",
        "* The performance difference between M1 and M2 could provide insights into the specific contribution of each additional component. A larger improvement due to M2 suggests that demographic and time information play a significant role."
      ],
      "metadata": {
        "id": "R1UqyC_d_U_k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH75TNU71eRH"
      },
      "source": [
        "# Discussion\n",
        "Even though we have used smaller fixed length cohort for pre-traninig, Our investigation into the TransformEHR model yielded promising results, suggesting its potential to become a valuable asset in predicting future patient diagnoses. This discussion will delve into the key findings, explore their implications for clinical practice, acknowledge limitations that pave the way for future advancements, and consider the original paper's reproducibility.\n",
        "\n",
        "**Superior Performance and Underlying Factors:**\n",
        "\n",
        "TransformEHR demonstrated a clear edge over existing models in predicting future visit ICD codes, particularly for uncommon diseases. This superiority can be attributed to several factors, as discussed previously.\n",
        "\n",
        "**Clinical Applications and Benefits:**\n",
        "\n",
        "The success of TransformEHR translates to several potential benefits in the clinical realm, as elaborated upon earlier.\n",
        "\n",
        "**Implications of the Experimental Results:**\n",
        "The findings of this study hold significant implications for the future of healthcare:\n",
        "* Early Disease Detection: The model's ability to predict uncommon diseases can lead to earlier diagnoses, potentially improving patient outcomes through timely intervention.\n",
        "* Personalized Medicine: By predicting multiple diagnoses simultaneously, TransformEHR can facilitate the development of personalized treatment plans that consider a patient's unique health profile.\n",
        "* Data-Driven Decision Making: The model can empower clinicians with data-driven insights to make informed decisions about patient care.\n",
        "\n",
        "**Reproducibility of the Original Paper:**\n",
        "\n",
        "Unfortunately, We could not reproduce the same model because of the original data unavailability. But we tried to replicate the architecture, masking the future visit so that model only focus on previous visits and can not cheat while predicting the icd_codes of future visit.\n",
        "\n",
        "Our pretraninig number's does not match with paper evaluation metrics because we have used MIMIC4 dataset for pre-tranining. Also, pre-training code is not avaialble. We have implemented the pre-training from scratch.\n",
        "\n",
        "Ideally, factors like the availability of code, data, and detailed methodological descriptions would be crucial for replication.\n",
        "\n",
        "**Challenges and Recommendations for Improved Reproducibility:**\n",
        "\n",
        "Here's a breakdown of potential difficulties encountered in replicating the study and recommendations for future research to enhance reproducibility:\n",
        "\n",
        " - **What Was Difficult:**\n",
        "\n",
        "   * Data Access: Large-scale EHR datasets can be challenging to obtain due to privacy concerns and institutional limitations. We have used MIMIC4 dataset.\n",
        "\n",
        "   * Computational Resources: Training deep learning models often requires significant computational power, which might not be readily available to all researchers.\n",
        "\n",
        "   * Code Availability: The absence of publicly available code made it difficult to replicate the exact implementation details of the model.\n",
        "\n",
        "   * Data Pre-Processing: We have spend most of the time on data pre-processing and identifing the correct way for attention masking. Data-Preprocessing diagram is confusing and can be further improved.\n",
        "      * Fig 2:How model learns the correlation of ICD codes by recovering the masked ICD codes to its original ICD codes.\n",
        "  \n",
        "   * Encoder & Decoder Layer Configuration: Paper does not talk about the Encoder & Decoder configuration such as number of layers used, number of head used etc. This can help in building better understanding of the overall architecture, capacity requirement for model training.\n",
        "\n",
        "\n",
        "- **What Was Easy:**\n",
        "\n",
        "   * Leveraging **PyHealh Framework** made it easy for us to pre-process MIMIC4 dataset.\n",
        "\n",
        "   * Understanding the Model Architecture: The discussion provided a clear explanation of the TransformEHR architecture (visit masking, encoder-decoder, time embeddings), facilitating comprehension for future studies.\n",
        "\n",
        "   * Evaluation Metrics: The use of established metrics (accuracy, AUROC, PPV) allows for easier comparison with other studies.\n",
        "\n",
        "**Recommendations:**\n",
        "\n",
        "* Open-source Code and Data (when possible): Sharing code and anonymized data (if feasible) would significantly enhance the reproducibility of the research.\n",
        "Detailed Methodological Descriptions: Providing comprehensive descriptions of the training process, hyperparameter tuning, and data preprocessing steps would aid in replication.\n",
        "\n",
        "* Containerization: Utilizing containerization tools (e.g., Docker) can ensure that the computational environment used for training is replicable.\n",
        "By following these recommendations, future research in this area can be more easily reproduced, fostering scientific progress and building trust in the development of machine learning models for healthcare applications.\n",
        "\n",
        "**Conclusion:**\n",
        "\n",
        "The TransformEHR model demonstrates remarkable promise for predicting future patient diagnoses.  Its superior performance, particularly for uncommon diseases, coupled with its potential clinical applications, make it a compelling tool for advancing healthcare practices. Addressing the limitations through further research and development, and prioritizing reproducibility through open science practices, can pave the way for the successful integration of TransformEHR into clinical workflows, ultimately leading to improved patient care.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project, focused on implementing the TransformEHR deep learning paper, yielded valuable learning experiences and achievements that will benefit our future endeavors:\n",
        "\n",
        "- Deep Learning's Power in Healthcare:\n",
        "\n",
        "  * We gained a deeper understanding of the immense potential that deep learning models hold for the healthcare field. The TransformEHR model's success in predicting future diagnoses showcases the power of deep learning for extracting meaningful insights from complex medical data.\n",
        "\n",
        "- Handling Medical Datasets:\n",
        "  * By working with a practical medical dataset, we acquired valuable hands-on experience in managing real-world healthcare data. This includes understanding the specific characteristics of medical data, addressing potential challenges like privacy concerns and data quality, and applying appropriate techniques for data manipulation.\n",
        "\n",
        "- Practical Skill Development:\n",
        "  * Throughout the project, we significantly improved our practical skills in several key areas:\n",
        "\n",
        "    * Data Preprocessing: We honed our ability to clean, transform, and prepare medical data for use in a deep learning model. Leveraging PyHealth framework made it easy for us.\n",
        "\n",
        "    * Model Building: We gained practical experience in designing and constructing deep learning models like TransformEHR, understanding the architecture and the choices involved in building such models.\n",
        "\n",
        "    * Model Training: We actively participated in the training process, learning the intricacies of training deep learning models, including hyperparameter tuning and addressing potential challenges.\n",
        "\n",
        "    * Model Evaluation: We developed a strong understanding of how to evaluate the performance of deep learning models in a healthcare context, utilizing relevant metrics like accuracy, AUROC, and PPV.\n",
        "\n",
        "Overall, this project has equipped us with a comprehensive understanding of deep learning's potential in healthcare, practical experience in handling medical data, and a refined skillset for data preprocessing, model building, training, and evaluation. These valuable assets will serve as a strong foundation for future explorations in deep learning applications for healthcare advancement."
      ],
      "metadata": {
        "id": "fG_vr88fVcvz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GitHub Repository\n",
        "\n",
        "https://github.com/satvikk2/CS598_DLH_Team88"
      ],
      "metadata": {
        "id": "KVmocDvaWtET"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHMI2chl9omn"
      },
      "source": [
        "# References\n",
        "\n",
        "1. Yang, Z., Mitra, A., Liu, W. et al. TransformEHR: transformer-based encoder-decoder generative model to enhance prediction of disease outcomes using electronic health records. Nat Commun 14, 7857 (2023). https://doi.org/10.1038/s41467-023-43715-z\n",
        "\n",
        "2. Vaswani, A. et al. Attention is All you Need. in Advances in Neural Information Processing Systems 30 (eds. Guyon, I. et al.) 5998–6008 (Curran Associates, Inc., 2017).https://arxiv.org/abs/1706.03762\n",
        "\n",
        "3. Rasmy, L., Xiang, Y., Xie, Z. et al. Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction. npj Digit. Med. 4, 86 (2021). https://doi.org/10.1038/s41746-021-00455-y\n",
        "\n",
        "4. Li, Y., Rao, S., Solares, J.R.A. et al. BEHRT: Transformer for Electronic Health Records. Sci Rep 10, 7155 (2020). https://doi.org/10.1038/s41598-020-62922-y\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}